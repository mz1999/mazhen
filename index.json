[{"categories":["tech"],"content":"TiDB Insert 执行流程图 TiDB源码解读系列的《Insert语句概览》讲解了Insert执行的整体流程，并在最后用一幅图描述了整个流程： 我按照自己的理解对这幅图扩展了一下，在原先数据结构转换流程的基础上，补充了代码的调用流程，个人感觉更加全面，希望对你阅读代码也有帮助。 ","date":"2022-05-15","objectID":"/insert-overview/:0:0","tags":["db"],"title":"TiDB Insert 执行流程图","uri":"/insert-overview/"},{"categories":["tech"],"content":"当TiDB 源码阅读系列更新到第六篇《Select 语句概览》时，我发现需要一些关系数据库的基础知识才能更好的理解，例如逻辑查询计划优化其实就是：使用代数定律对查询语句的代数表达式树做等价转换，使改进后的代数表达式树预期可以生成更有效的物理查询计划。有了这些基础知识，看代码才能做到知其然知其所以然。本文希望通过梳理关系数据库背后的知识，为读懂 TiDB 查询处理器部分的源码扫清障碍。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:0:0","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"极简数据库发展史 数据库的应用及其广泛，已经成为信息系统的核心技术和重要的基础设施。简单说数据库需要做两件事：存储数据，以及随后在你需要的时候能访问读取数据。 最早的数据库是基于文件系统，虽然它满足了长期存储数据的需求，但没有提供对文件的查询语言，读取访问非常不便利。于是人们在文件系统上引入一层抽象：数据模型。数据模型是对现实世界数据特征的抽象，能比较真实地模拟现实世界，容易为人所理解，也便于在计算机上实现。 最早出现的是层次模型（Hierarchical Model），数据被组织为一棵树，类似于今天文档数据库使用的JSON的结构。层次模型很适合处理one-to-many关系，但要表现many-to-many关系则非常困难，一般也不支持join。使用层次模型最著名的数据库是 IBM 的Information Management System (IMS)，它最初是为了解决阿波罗飞船登月计划的需求，协调分散在全球制造的200万个阿波罗飞船零部件的生产进度。 随后出现了不同的方案解决层次模型的限制，其中最突出的两个模型是网络模型（Network Model）和关系数据模型，最终关系数据模型胜出。 今天最著名和使用最广泛的数据模型是由 Edgar Codd 博士提出的关系数据模型，他在1970年发布的论文《A Relational Model of Data for Large Shared Data Banks》，奠定了关系数据库的理论基础。ACM在1983年把这篇论文列为从1958年以来的四分之一世纪中具有里程碑式意义的最重要的25篇研究论文之一。到了80年代中期，基于关系数据模型的关系数据库已经成为人们存储、查询结构化数据的首选工具。 到了2010年，NoSQL兴起，试图颠覆关系数据模型的统治地位。随着互联网的爆发式发展，数据库领域又一次发生了摇摆，伴随着互联网的特殊需求，一批有着新鲜血液的 NoSQL 数据库涌现了出来，层次模型又重新站在了大家面前。NoSQL为了应对海量数据存储和高并发访问，决定放弃关系数据模型和事务等关系数据数据库的关键特性。自从 NoSQL 概念横空出世，关系数据库似乎成了低效、高成本、速度慢的数据处理模式的代名词。然而，NoSQL在解决问题的同时也给使用者带来了很多困扰， 最终一致让应用开发者要面对各种复杂的场景。 数据库技术的发展是螺旋式上升，Google发布的Spanner和F1两篇论文，让人们看到了关系数据模型 和 NoSQL 融合的可能性。以 TiDB 为代表的 NewSQL 数据库，让人们重新享受关系模型、强一致性事务等对使用者友好的特性，同时也具备了 NoSQL 的水平扩展能力。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:1:0","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"关系数据模型 和 关系代数 数据模型是对现实世界事物的抽象，而关系数据模型将一切事物都抽象为关系，并通过集合运算的方式规定了关系之间的运算过程，模型相对的比较简单，数据证明严谨，因此很快被大家广泛接受。 这一节我将介绍关系数据库的数学基础：关系数据模型和关系代数。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:2:0","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"关系数据模型 关系模型为人们提供了一种描述数据的方法：一个称为关系（relation）的二维表。现实世界的实体以及实体间的各种联系都可以用关系来表示。我们通过例子来了解关系模型的重要术语： 雇员表 emp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 … … … … … 关系（Relation） ：一个关系对应通常说的一张二维表 元组（Tuple） ： 表中的一行即为一个元组 属性（Attribute） ：表中的一列即为一个属性，给每一个属性起一个名称即属性名 键（Key）：表中的某个属性组，它可以唯一确定一个元组 域（Domain） : 是一组具有相同数据类型的值的集合。属性的取值范围来自某个域。例如性别的域是（男，女）。 关系模式（schema）：对关系的描述，先给出一个关系名，其后是用圆括号扩起来的所有属性，例如：employees（emp_no, name, birth_date, gender, hire_date） ","date":"2018-07-01","objectID":"/rdbms-fundamental/:2:1","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"关系代数 一门代数是由一些操作符和操作数组成。例如算术代数的加、减、乘、除是操作符，变量x和常量8是操作数。任何一门代数都允许把操作符作用在操作数上构造出表达式（expression），例如算术表达式 (x+y)*3。 关系代数 也是一门代数，它的操作数是关系，操作运算符有两类：集合运算符和专门的关系运算符。 关系代数可以认为是一种抽象的查询语言，利用对关系的运算来表达查询，运算对象是关系，运算结果也是关系。因此，关系代数的表达式也被称为查询（query）。 传统的集合运算 三个最常见的集合操作是：并（union）、交（intersection）、差（difference）。 R ∪ S，表示关系R和S的并，得到的结果关系的元素来自R或者S，或R和S中都出现过。 R ∩ S，表示关系R和S的交，同时在R和S中存在的元素集合。 R - S，表示关系R和S的差，它是由在R中出现但不在S中出现的元素构成的集合。 另外一个集合操作是笛卡尔积（Cartesian Product）。 关系R和S的笛卡尔积是一个有序对的集合，有序对的一个元素是关系R中的任何一个元组，第二个元素是关系S中的任何一个元组表示为 R × S。 关系R A B 1 2 3 4 关系S B C D 2 5 6 4 7 8 9 10 11 R × S的结果 A R.B S.B C D 1 2 2 5 6 1 2 4 7 8 1 2 9 10 11 3 4 2 5 6 3 4 4 7 8 3 4 9 10 11 专门的关系运算 选择（selection)，当选择操作符应用到关系R上时，产生一个关系R的元组的子集合。结果关系元组必须满足某个涉及R中属性的条件C，表示为 σC( R ) 投影 （projection），用来从一个关系生成一个新的关系，这个关系只包含原来关系R中的部分列。表达式 πA1,A2,…,An ( R ) 的值是这样一个关系，它只包含关系R属性A1,A2,...An所代表的列。 θ连接，关系R和关系S满足条件C的θ连接可以用这样的符号来表示： R ⋈C S θ连接的结果这样构造： 先得到R和S的笛卡尔积 在得到的关系中寻找满足条件C的元组 关系R A B C 1 2 3 6 7 8 9 7 8 关系S B C D 2 3 4 2 3 5 7 8 10 R ⋈ A\u003cD AND R.B ≠ S.B S 的结果是： A R.B R.C S.B S.C D 1 2 3 7 8 10 有两类常用的连接运算： 等值连接（equijoin）：比较运算符为 = 的连接运算称为等值连接。例如： R ⋈ R.A = S.B S 是从关系R与S的笛卡尔积中选取A、B属性值相等的那些元组。 自然连接（Natural join）：自然连接是一种特殊的等值连接，两个关系中进行比较的分量必须是相同的属性组，并在结果中把重复的属性列去掉。关系R和S的自然连接表示为 R ⋈ S 关系R A B 1 2 3 4 关系S B C D 2 5 6 4 7 8 9 10 11 R ⋈ S A B C D 1 2 5 6 3 4 7 8 两个关系R和S在做自然连接时，如果一个元组不能和另外关系中的任何一个元组配对的话，这个元组被称为悬浮元组（Dangling tuple）。上面的例子中，关系S的第三个元组就是悬浮元组。 如果把悬浮元组也保存在结果关系中，而在其他属性上填空值(Null)，就叫做外连接（Outer Join）。 左外连接(LEFT OUTER JOIN或LEFT JOIN)：只保留左边关系R中的悬浮元组 右外连接(RIGHT OUTER JOIN或RIGHT JOIN)：只保留右边关系S中的悬浮元组 关系代数的扩展操作符 消除重复操作符（duplicated-elimination operator）用 δ(R) 来返回一个没有重复元组的关系R 聚集操作符 （aggregation operator）用来汇总或者聚集关系某一列中出现的值，有 SUM，AVG，MIN，MAX，COUNT 等 分组操作（grouping）根据元组在一个或多个属性上的值把关系的元组拆成“组”。这样聚集操作就可以对分组的各个列进行计算。分组操作符 γ 是组合了分组和聚合操作的一个算子。例如表达式： γ gender, COUNT(emp_no)-\u003ecount(employees) 代表把性别（gender）作为分组属性，然后对每一个分组进行COUNT(emp_no)的操作。 排序算子（sorting operator）如果关系R的模式是 R(A,B,C)，那么 τC( R ) 就把R中的元组按照属性C的值排序。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:2:2","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"关系代数小结 上面的知识有些枯燥，但非常容易理解，因为我们经常使用关系数据库，已经接受了这些概念。掌握了一些关系代数的知识，在阅读TiDB源码时，当看到selection、projection 这些术语就能一下想到它们对应的关系代数运算符。 这里只介绍了关系代数最基本的概念，如果想完整学习，建议参考斯坦福大学大学的课程CS145: A First Course in Database Systems，对应的教材有中文版《数据库系统基础教程》。 其实我们在查询时提交给数据库的就是关系代数表达式，它是关系运算符的组合，数据库会根据一些代数定律对最初的表达式做等价变换，得出一个最优的等价表达式（equivalent expression），即可以更快的被计算出结果的表达式。这个过程就是逻辑查询计划优化，后面我会简单的介绍相关概念。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:2:3","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"SQL 的诞生 SQL(Structured Query Language) 结构化查询语言，是关系数据库的标准语言。 在1970年Codd博士提出了关系模型之后，由于关系代数或者关系都太数学了，难以被普通用户接受。IBM在研制关系数据库管理系统原型System R的过程中，决定摈弃数学语言，以自然语言为方向，结果诞生了结构化英语查询语言（Structured English Query Language，SEQUEL），后来更名为SQL。System R 因此获得1988年度ACM“软件系统奖”。 SQL是声明式查询语言，你只需要指定想要获得什么样的数据，而无须了解如何实现这个目标。SQL具体是如何执行的，取决于数据库系统的查询处理器，它来决定哪些索引和哪些连接方法可以使用，以及以什么样的顺序执行查询的各个部分。SQL隐藏了数据库引擎的实现细节，因此用户可以在不修改查询语句的情况下，享受到数据库性能优化带来的好处。 下面我们来看看数据库的查询处理器。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:3:0","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"关系数据库的查询处理器 SQL是在很高层次上表达查询，那么数据库的查询处理器必须提供查询被如何执行的大量细节。下面我从概念上介绍查询处理器的处理流程，实际的数据库实现要复杂的多，特别是像 TiDB 这样的分布式数据库。如果想比较系统的了解数据库的实现技术，同样推荐斯坦福大学计算机科学专业的课程 CS245: Database System Implementation。上面提到的CS145是CS245的预修课。国内很少有讲数据库内部实现的书，这门课的教材值得阅读。当然最好的学习方法是理论联系实践，多去读 TiDB 的源代码:) 一般查询处理可以简单的划分为以下几个步骤： 对SQL进行语法分析，将查询语句转换成抽象语法树。 把抽象语法树转换成关系代数表达式树，这就是初始的逻辑查询计划。 使用关系代数中的多个代数定律改进初始的代数表达式树。利用一些代数定律，可以把代数表达式树转换成一个等价的表达式树，后者预期可以生成更有效的物理查询计划。这一步进行了查询重写，可以称为逻辑查询计划优化。 为逻辑查询计划的每一个操作符选择实现算法，并确定这些操作符的执行顺序，逻辑查询计划被转化为物理查询计划。物理查询计划指明了要执行的操作，操作的执行顺序，执行每步所用的算法，获取数据的方式，以及数据从一个操作传递给另一个操作的方式。 ","date":"2018-07-01","objectID":"/rdbms-fundamental/:4:0","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"查询示例 本文准备以一个简单的例子来介绍查询处理的流程，下面是查询涉及的两张表： employees emp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 … … … … … salaries emp_no salary last_modified 1 8000 2018-04-01 2 6000 2018-04-01 3 15000 2018-04-01 … … … 想要获取工资大于7000的员工姓名列表，SQL语句可以这么写： SELECT name FROM employees, salaries WHERE employees.emp_no = salaries.emp_no AND salary \u003e 7000; ","date":"2018-07-01","objectID":"/rdbms-fundamental/:4:1","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"SQL 语法分析 SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST）。具体的实现可以参考这篇文章《TiDB SQL Parser 的实现》。 示例SQL解析完成后得到下面的语法树： ","date":"2018-07-01","objectID":"/rdbms-fundamental/:4:2","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"生成逻辑查询计划 现在将上一步生成的语法树转换成关系代数表达式树，也就是逻辑查询计划。对于示例SQL的抽象语法树转换过程如下： \u003cFromList\u003e 中涉及的关系employees和salaries做笛卡尔积运算 选择（selection）运算 σC，其中C被替换成\u003cCondition\u003e表达式，即employees.emp_no = salaries.emp_no AND salary \u003e 7000 投影（projection） πL，其中L是\u003cSelList\u003e中的属性列表，对于这个查询只有一个属性name 我们得到下面的关系代数表达式树： ","date":"2018-07-01","objectID":"/rdbms-fundamental/:4:3","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"逻辑查询计划的改进 当我们把查询语句转换成关系代数表达式时，得到了一个初始的逻辑查询计划。现在我们可以使用关系代数中的多个代数定律改进逻辑查询计划。 这里仅仅列出一小部分这样的代数定律，它们可以将一个表达式树转换成一个等价的表达式树。 交换律和结合律 R × S = S × R; (R X S) × T = R × (S × T) R ⋈ S = S ⋈ R; (R ⋈ S) ⋈ T = R ⋈ (S ⋈ T) R ∪ S = S ∪ R; (R ∪ S) ∪ T = R ∪ (S ∪ T) R ∩ S = S ∩ R; (R ∩ S) ∩ T = R ∩ (S ∩ T) 涉及选择的定律 σC1 AND C2 = σC1(σC2( R )) σC1 OR C2 = (σC1( R ) ∪ (σC2( R ) ) σC1(σC2( R )) = σC2(σC1( R )) 下推选择 在表达式中下推选择是查询优化器最强有力的工具。 σC( R × S ) = σC( R ) × S σC( R ⋈ S) = σC( R ) ⋈ S σC( R ⋈D S) = σC( R ) ⋈D S 涉及连接和笛卡尔积的定律 R ⋈C S = σC( R × S) 涉及消除重复的定律 δ(R×S) = δ( R ) × δ(S) δ(R⋈CS) = δ( R ) ⋈C δ(S) 另外还有涉及投影的定律、涉及分组和聚集的定律。这部分有些理论化，可以参考这篇《TiDB 源码阅读系列文章（七）基于规则的优化》看看 TiDB 具体是怎么做的。 对于本例使用到的定律比较简单。先将选择的两部分分解为 σemployees.emp_no = salaries.emp_no 和 σsalary \u003e 7000，后者可以在树中下推。第一个条件涉及笛卡尔积两边的属性，可以把上面提到的定律 R ⋈C S = σC( R × S) 从右向左使用，把笛卡尔积转换成连接。使用了两个定律后，得到优化后的逻辑查询计划如下图： ","date":"2018-07-01","objectID":"/rdbms-fundamental/:4:4","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"物理查询计划的生成 这一步我们需要把逻辑查询计划转换成物理查询计划。通常由逻辑计划可以得到多个物理计划，我们需要对每个物理计划进行评估，选择具有最小估计代价的物理查询计划。 基于代价的物理计划选择 在从逻辑计划构建物理计划的时候，因为可能得到多个物理计划，我们需要通过估计物理计划执行的代价来确定最优选择。 如何计算这个代价呢？我们可以用物理计划每一步的任务执行时发生的磁盘I/O数、网络吞吐量、占用的内存空间大小等近似估算。 这些资源的访问和占用，又是由什么决定的呢？可能包括的决定因素有： 参与运算任务执行的数据大小 数据的分布位置（连续的磁盘空间、离散的磁盘空间、网络节点等） 关系中属性的不同值的数目 属性值的最大值、最小值、以及值的分布情况 数据库会收集统计这些信息，用来估算具体任务的代价。 逻辑查询计划在转换成物理计划的时候，每一步的转换都会面临多种情况的选择，最容易想到的是使用穷举法，估算每一种情况的代价，最后确定最优的物理计划。但使用穷举法的话，很可能估算本身的代价变得非常大，实践中可以采用动态规划（dynamic programming）等算法。 枚举物理查询计划 以上一步输出的逻辑查询计划为例，看看在枚举物理查询计划时需要做出哪些选择。 首先，逻辑查询计划的每个结点转换成什么样的物理运算符会遇到多种选择。我们从逻辑查询计划树自底往上来看： 逻辑计划的叶子结点 逻辑查询计划树的叶子结点被一个扫描运算符替代。这些运算符一般包括： TableScan( R )：以任意顺序读人所有元组 SortScan(R, L)：按照顺序读入R的元组，并以列L中的属性进行排列 IndexScan(R, C)：C是一个带有比较运算符的条件，例如 A = 100，A是R的一个属性，如果A上建立的索引，可以通过索引来访问R的元组。如果比较运算符不是等值比较，则索引必须是一个支持范围查询的索引，例如B+ Tree IndexScan(R, A)：这里A是R的一个属性，关系R通过A上的索引被检索。 如果R的数据在磁盘上不是占用连续的存储空间，该运算符可能比TableScan更有效。 逻辑计划的σ选择（selection)结点 σ选择结点一般有两种选择： 可以简单的用物理过滤运算符Filter( C )替代 σC( R ) 如果C是一个带有比较运算符的条件，例如 A = 100，并且属性A上有索引，可以把比较运算合并到扫描运算符：IndexScan(R, A = 100)。 对于本例，salary 一般都不会建立索引，因此可以把σ( salary \u003e 7000 ) 替换为 Filter( salary \u003e 7000 ) 逻辑计划的连接结点 常见的等值连接满足结合律和交换律，因此连接可以得到很多候选的物理执行计划。其中最关键的问题是确定连接顺序。 当两个关系连接，只有两种选择，一般我们应该将估计值较小的关系放在前面。当连接有2个以上关系时，可能的连接树的数量会迅速增加，例如4个关系的连接将会有4!=24种连接方式。这一部分很复杂，就不在本文讨论了。 除了连接顺序，还需要确定具体使用的连接算法。常见的算法有： nested loops 基于排序的join（sort merge join） hash join 基于索引的join 逻辑计划的投影结点 投影结点的任务比较明确，输出包含指定列的关系。 除了将逻辑查询计划的结点转换成物理运算符，在选择物理计划时还要考虑数据如何在运算符之间流动（中间结果保存到磁盘或流水线方式），物理运算符的执行顺序等。这些细节本文就不再讨论。 最后，假定我们在所有选择的组合中，确定了其中一个作为最优的物理查询计划，然后就可以把它交给查询执行器真正的执行了： ","date":"2018-07-01","objectID":"/rdbms-fundamental/:4:5","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"写在最后 本文把关系数据库查询处理涉及的基础知识进行了梳理，希望对你理解 TiDB 的代码能有所帮助。 数据库是一个非常迷人的领域，它有很强的理论基础，同时又涉及大量的工程实践，可以说是最复杂的系统软件之一。我相信能开发数据库是很多程序员的梦想。梦想还是要有的，让我们一起努力吧！ ","date":"2018-07-01","objectID":"/rdbms-fundamental/:5:0","tags":["db"],"title":"关系数据库查询处理基础知识扫盲","uri":"/rdbms-fundamental/"},{"categories":["tech"],"content":"为什么XA事务建议用SERIALIZABLE隔离级别 在MySQL最新的官方文档中，关于XA Transactions的介绍有这么一段描述： As with nondistributed transactions, SERIALIZABLE may be preferred if your applications are sensitive to read phenomena. REPEATABLE READ may not be sufficient for distributed transactions. 这段话表达的意思是，对于分布式XA事务， REPEATABLE READ 隔离级别是不够的。 MySQL旧版本的文档，对于这个问题表达的更加直接： However, for a distributed transaction, you must use the SERIALIZABLE isolation level to achieve ACID properties. It is enough to use REPEATABLE READ for a nondistributed transaction, but not for a distributed transaction. 怎么理解呢？举个简单的例子：假设MySQL使用的是REPEATABLE READ 隔离级别，XA事务 T1 修改的数据涉及两个节点 A 和 B，当事务 T1 在 A 上完成commit，而在 B 上还没commit之前，也就是说这时事务 T1 并没有真正结束，另一个XA事务 T2 已经可以访问到 T1 在 A 上提交后数据，这不是出现脏读了吗？ 那么使用SERIALIZABLE就能保证吗？还是看例子：事务 T1 修改节点 A 上的数据 a -\u003e a'，修改 B 上的数据 b -\u003e b'，在提交阶段，可能被其他事务 T2 读取到了 a'， 因为使用了SERIALIZABLE隔离级别， MySQL 会对所有读加锁，那么 T2 在 B 上读取 b 时会被一直阻塞，直到 T1 在 B 上完成commit，这时 T2 在 B 读取到的就是 b'。 也就是说，SERIALIZABLE隔离级别保证了读到 a' 的事务，不会读到 b ，而是读到 b'，确保了事务ACID的要求。 更加详细的描述可以参考鹅厂 TDSQL XA 事务隔离级别的奥秘，他们的结论是： 如果某个并发事务调度机制可以让具有依赖关系的事务构成一个有向无环图(DAG)，那么这个调度就是可串行化的调度。由于每个后端DB都在使用serializable隔离级别，所以每个后端DB上面并发执行的事务分支构成的依赖关系图一定是DAG。 只要所有连接都是用serializable隔离级别，那么TDSQL XA执行的事务仍然可以达到可串行化隔离级别。 ","date":"2018-06-05","objectID":"/mysql-xa/:1:0","tags":["db"],"title":"关于MySQL XA事务的隔离级别","uri":"/mysql-xa/"},{"categories":["tech"],"content":"SERIALIZABLE性能差，有更好的实现方式吗 如果分布式事务想实现read-committed以上的隔离级别，又不想使用SERIALIZABLE，有什么更好的方式吗？ 当然有，想想看TiDB是怎么做的，底层TiKV是一个整体，有全局的MVCC，所以能够做到分布式事务的Snapshot隔离级别。 PostgreSQL社区中，有Postgres-XC和Postgres-XL的方案，采用的并发机制是全局MVCC 和本地写锁。 Postgres-XC 维持了全局活跃事务列表，从而提供全局MVCC。 虽然MySQL也实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，无法得到全局一致性快照，自然就很难做到分布式的Snapshot隔离级别。腾讯的这篇文章也分析了这么做比较困难： 由于MySQL innodb使用MVCC做select（除了serializable和for update/lock in share mode子句），还需要将这个全局事务id给予innodb做事务id，同时，还需要TDSQL XA集群的多个set的innodb 共享各自的本地事务状态给所有其他innodb（这也是PGXL 所做的），任何一个innodb的本地事务的启动，prepare，commit，abort都需要通知给所有其他innodb实例。只有这样做，集群中的每个innodb实例才能够建立全局完全有一致的、当前集群中正在处理的所有事务的状态，以便做多版本并发控制。 这本身都会造成极大的性能开销，并且导致set之间的严重依赖，降低系统可靠性。这些都是我们要极力避免的。 ","date":"2018-06-05","objectID":"/mysql-xa/:2:0","tags":["db"],"title":"关于MySQL XA事务的隔离级别","uri":"/mysql-xa/"},{"categories":["tech"],"content":"结论 根据上面的分析，如果使用MySQL 的 XA分布式事务，最安全的方式还是按照官方建议，使用SERIALIZABLE隔离级别。 如果想基于MySQL做改造，实现全局MVCC，从而实现分布式事务的Snapshot隔离级别，目前还没有看到MySQL社区有这类项目，相信实现难度比较大。 ","date":"2018-06-05","objectID":"/mysql-xa/:3:0","tags":["db"],"title":"关于MySQL XA事务的隔离级别","uri":"/mysql-xa/"},{"categories":["tech"],"content":"现在新出现的MySQL中间件除了基本的数据sharding功能，都增加了很多高级特性，我觉得有三个特性比较重要： 分布式事务的支持 数据的强一致复制，提高了数据的安全性和可用性 支持跨shard join 通过对这些特性的支持，MySQL中间件具备了一些newSQL数据库的能力，不再是个纯粹的中间件，让用户更容易使用。我调研了最近开源的青云RadonDB，希望了解下这方面最新的进展。 先简单看下RadonDB的整体架构：存储/计算分离。存储节点是MySQL，3个一组用raft实现数据的强一致性，不再是异步/半同步复制，数据的安全性、可用性级别更高。上层是SQL节点，负责客户端连接、SQL语句的解析和执行、分布式事务协调、数据sharding逻辑等。右下脚计算节点的作用，后面会解释。 在知乎上看到\u003c如何评价青云开源的分布式数据库 radondb\u003e，RandoDB被吐槽的很厉害。我们从这些吐槽可以了解产品宣传之外的一些信息，知道做这种中间件不是那么容易。大家对RandoDB的几个关键特性的实现方式都不太满意。让我们逐一看看。 分布式事务的实现 对分布式事务的实现大家吐槽的最厉害： 官方宣传用XA实现了Snapshot Isolation，然而众所周知XA是无法实现SI的。所谓的事务其实只支持单条SQL，BEGIN / SET AUTOCOMMIT=0 都不支持。 单语句事务，就是不能 begin 开启事务。 为了达到 SI 隔离级别，在执行用户 SQL 时，会加上一个 commitLock，防止其他事务提交。这决定了加锁必须时间很短，比如一条SQL，如果你从start transaction开始加锁，那其他事务全都无法提交了，系统事实上已经不可用。 所谓分布式事物快照隔离级别是 radondb 层 query 和 commit 语句串行化实现的。这个应该是串行化隔离级别了。而且是和冲突没关系的串行化，就是说根本不管两个事物之间有没有冲突数据。性能自行脑补。 没有 XA log 的 XA 事务原子性实现都是耍流氓。 为什么XA是无法实现SI？ 我的理解是，单个MySQL实例虽然实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，每个实例内部的MVCC是独立的，无法得到全局一致性快照。XA事务跨越了多个节点，所以没办法实现Snapshot隔离级别。可以对比下TiDB的实现，底层TiKV是一个整体，有全局的MVCC，所以能在上层支持分布式事务的Snapshot隔离级别。 RandoDB的实现能work，但相当于在Proxy层将所有事务串行化，即使两个事务之间没有数据冲突。而且只有单语句事务。 对于XA log，开发者的解释是： proxy xa log只针对xa commit出错，目前通过分析log然后人工介入，这里没有再记log的必要 我觉得这么做很不严谨。2PC协议有一个问题，一旦事务参与者投票，它必须等待coordinator给出指示，提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery。因此，现在很多改进的做法是用Paxos/raft保证事务日志的高可用，coordinator挂了可以快速切换。即使不用raft，找一个地方可靠持久的保存事务日志是非常必要的。 使用Raft保证强一致性 现在很多项目都会使用Paxos/Raft来改进MySQL的复制机制，实现数据的强一致性。如果主、备间任何时刻都完全一致，那么任何时刻都可以安全的进行主备切换。如果无法保证主、备间的强一致性，那么当有持续不断的更新时，主备切换就无法保证强一致性，需要在切换时暂停主库的写入，服务会有短暂的中断。 腾讯的PhxSQL就是建立在Paxos的一致性和MySQL的binlog流水基础上，通过Paxos保证强一致和高可用的MySQL集群。关于PhxSQL的设计理念可以参见： 谈谈PhxSQL的设计和实现哲学（上） 谈谈PhxSQL的设计和实现哲学（下） 采用类似做法的还有阿里云的MySQL金融版。另外，MySQL官方也从5.7开始推出了Group Replication机制，内部使用Paxos实现了多个副本的一致。 RadonDB的实现机制和PhxSQL不太一样。它在一组MySQL集群内的复制还是通过Semi-Sync机制（Semi-Sync设置为无限大，不退化为异步复制），保证有一个slave和master完全一致。主备切换时会选择这个slave为主，然后结合MySQL的 Multi-threaded replication 和 GTID机制 进行快速回放，让主备重新一致。Raft用在哪里了？在 RadonDB 只使用 Raft 进行选主，当主挂掉之后，使用 Raft 选出新的主。Raft选主的逻辑是选出一个拥有最多的committed log的成员作为主，那么对于RadonDB来说，哪个MySQL的GTID最大就选哪个。 我自己还没有使用Raft的经验，不确定RadonDB的实现机制是否合理。但利用Semi-Sync模拟同步复制的方案，我觉得有一个地方不妥。当和主库保持强同步的备库有问题时，这组MySQL整体就不可用，因为它需要至少一个备库和主库完全一致，这就因为单点降低了整个集群的可用性。如果是用Raft做数据复制，就不会有这种单点影响全局可用性的问题。 另外，RandoDB被吐槽 Raft 的实现业余、不严谨： 打开用来做HA的Xenon模块，一看又是作业级的肯写代码不肯写测试的raft练手实现。raft测试用例一共1500行go代码 刚才数了下，自己的raft库光election相关的单元测试用例就数千行代码了。做生产环境用的系统不是练手写作业，需要一个go的raft库，既然都不肯写完备的测试了，那就老老实实用etcd或者hashicorp的raft。自己私下撸一个raft库练手，和给自己全职项目选一个可靠的raft实现，两者真的不矛盾。最滑稽，只做选主干嘛自己撸一个raft实现？ join等复杂查询的实现 严格说RandoDB是不支持join的。它的做法是让计算节点通过binglog复制了全量数据，SQL节点会把join等复杂查处路由到计算节点执行。 “计算节点”使用tokudb存储引擎存储全量数据，为了支持复杂查询。。。如果我一个分布式系统的数据总量有20T、100T，也用单个“计算节点”存储全量数据？而且这个数据同步过程是异步的，显然没法用在OLTP场景。 通过一些实用的方式支持了Join，这种做法可以work，但RandoDB离它宣称的数据库还差很远，缺少全局的执行计划优化。 总体来说，RandoDB的理想很宏大，用实用的方案解决了一些问题，但要成为真正成熟的数据库产品还差的比较远。RadonDB 的核心代码1万行左右。加上其它类库引入，Radon代码11万+， Xenon代码5万行+ 。 最后，看到有人推荐腾讯的TDSQL，也顺便了解了一下。从资料看TDSQL很不错，可惜不是开源产品。除了水平扩张、安全增强、自动化运维以外，它具备了我们上面提到的数据库中间件的高级特性： 支持分布式事务XA 全局事务的隔离级别最高可以达到serializable级别 分布式事务对业务透明，兼容单机事务语法 允许事务中多条语句分别发给多个分片 支持autocommit下单条语句写访问多个分片 默认采用强同步复制，即主从节点数据完全一致 复杂查询方面 允许以流式处理方式运行group by、order by 支持两个Shard使用shardkey（分表键）做等值连接，以及使用shardkey的子查询 支持了部分受限的复杂查询，对于数据库中间件来说已经算比较强大了。关于TDSQL的分布式事务，可以通过这两篇进行更多的了解： 一文教你迅速解决分布式事务 XA 一致性问题 鹅厂 TDSQL XA 事务隔离级别的奥秘 如果我们做MySQL中间件，可以瞄准TDSQL，对于分布式事务、数据强一致性，以及复杂查询、跨shard join 等特性都要考虑支持。 ","date":"2018-06-03","objectID":"/radondb/:0:0","tags":["db"],"title":"从RadonDB看新型数据库中间件的特性","uri":"/radondb/"},{"categories":["tech"],"content":" alt : Cursor功能实现总结.pdf ","date":"2018-05-11","objectID":"/cursor-summary/:0:0","tags":["db"],"title":"Cursor功能实现总结","uri":"/cursor-summary/"},{"categories":["tech"],"content":"PingCAP发布了TiDB的源码阅读系列文章，让我们可以比较系统的去学习了解TiDB的内部实现。最近的一篇《SQL 的一生》，从整体上讲解了一条SQL语句的处理流程，从网络上接收数据，MySQL协议解析和转换，SQL语法解析，查询计划的制定和优化，查询计划执行，到最后返回结果。 其中，SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST），这部分功能需要些背景知识才能比较容易理解，我尝试做下相关知识的介绍，希望能对读懂这部分代码有点帮助。 TiDB是使用goyacc根据预定义的SQL语法规则文件parser.y生成SQL语法解析器。我们可以在TiDB的Makefile文件中看到这个过程，先build goyacc工具，然后使用goyacc根据parser.y生成解析器parser.go： goyacc: $(GOBUILD) -o bin/goyacc parser/goyacc/main.go parser: goyacc bin/goyacc -o /dev/null parser/parser.y bin/goyacc -o parser/parser.go parser/parser.y 2\u003e\u00261 ... goyacc是yacc的Golang版，所以要想看懂语法规则定义文件parser.y，了解解析器是如何工作的，先要对Lex \u0026 Yacc有些了解。 ","date":"2018-05-09","objectID":"/sql-parser/:0:0","tags":["db"],"title":"TiDB SQL Parser 的实现","uri":"/sql-parser/"},{"categories":["tech"],"content":"Lex \u0026 Yacc 介绍 Lex \u0026 Yacc 是用来生成词法分析器和语法分析器的工具，它们的出现简化了编译器的编写。Lex \u0026 Yacc 分别是由贝尔实验室的Mike Lesk 和 Stephen C. Johnson在1975年发布。对于Java程序员来说，更熟悉的是ANTLR，ANTLR 4 提供了 Listener+Visitor 组合接口， 不需要在语法定义中嵌入actions，使应用代码和语法定义解耦。Spark的SQL解析就是使用了ANTLR。Lex \u0026 Yacc 相对显得有些古老，实现的不是那么优雅，不过我们也不需要非常深入的学习，只要能看懂语法定义文件，了解生成的解析器是如何工作的就够了。我们可以从一个简单的例子开始： 上图描述了使用Lex \u0026 Yacc构建编译器的流程。Lex根据用户定义的patterns生成词法分析器。词法分析器读取源代码，根据patterns将源代码转换成tokens输出。Yacc根据用户定义的语法规则生成语法分析器。语法分析器以词法分析器输出的tokens作为输入，根据语法规则创建出语法树。最后对语法树遍历生成输出结果，结果可以是产生机器代码，或者是边遍历 AST 边解释执行。 从上面的流程可以看出，用户需要分别为Lex提供patterns的定义，为 Yacc 提供语法规则文件，Lex \u0026 Yacc 根据用户提供的输入文件，生成符合他们需求的词法分析器和语法分析器。这两种配置都是文本文件，并且结构相同： ... definitions ... %% ... rules ... %% ... subroutines ... 文件内容由 %% 分割成三部分，我们重点关注中间规则定义部分。对于上面的例子，Lex 的输入文件如下： ... %% /* 变量 */ [a-z] { yylval = *yytext - 'a'; return VARIABLE; } /* 整数 */ [0-9]+ { yylval = atoi(yytext); return INTEGER; } /* 操作符 */ [-+()=/*\\n] { return *yytext; } /* 跳过空格 */ [ \\t] ; /* 其他格式报错 */ . yyerror(\"invalid character\"); %% ... 上面只列出了规则定义部分，可以看出该规则使用正则表达式定义了变量、整数和操作符等几种token。例如整数token的定义如下： [0-9]+ { yylval = atoi(yytext); return INTEGER; } 当输入字符串匹配这个正则表达式，大括号内的动作会被执行：将整数值存储在变量 yylval 中，并返回 token 类型 INTEGER 给 Yacc。 再来看看 Yacc 语法规则定义文件： %token INTEGER VARIABLE %left '+' '-' %left '*' '/' ... %% program: program statement '\\n' | ; statement: expr { printf(\"%d\\n\", $1); } | VARIABLE '=' expr { sym[$1] = $3; } ; expr: INTEGER | VARIABLE { $$ = sym[$1]; } | expr '+' expr { $$ = $1 + $3; } | expr '-' expr { $$ = $1 - $3; } | expr '*' expr { $$ = $1 * $3; } | expr '/' expr { $$ = $1 / $3; } | '(' expr ')' { $$ = $2; } ; %% ... 第一部分定义了 token 类型和运算符的结合性。四种运算符都是左结合，同一行的运算符优先级相同，不同行的运算符，后定义的行具有更高的优先级。 语法规则使用了BNF定义。BNF 可以用来表达上下文无关（context-free）语言，大部分的现代编程语言都可以使用 BNF 表示。上面的规则定义了三个产生式。产生式冒号左边的项（例如 statement）被称为非终结符， INTEGER 和 VARIABLE 被称为终结符,它们是由 Lex 返回的 token 。终结符只能出现在产生式的右侧。可以使用产生式定义的语法生成表达式： expr -\u003e expr * expr -\u003e expr * INTEGER -\u003e expr + expr * INTEGER -\u003e expr + INTEGER * INTEGER -\u003e INTEGER + INTEGER * INTEGER 解析表达式是生成表达式的逆向操作，我们需要归约表达式到一个非终结符。Yacc 生成的语法分析器使用自底向上的归约（shift-reduce）方式进行语法解析，同时使用堆栈保存中间状态。还是看例子，表达式x + y * z的解析过程： 1 . x + y * z 2 x . + y * z 3 expr . + y * z 4 expr + . y * z 5 expr + y . * z 6 expr + expr . * z 7 expr + expr * . z 8 expr + expr * z . 9 expr + expr * expr . 10 expr + expr . 11 expr . 12 statement . 13 program . 点（.）表示当前的读取位置，随着 . 从左向右移动，我们将读取的token压入堆栈，当发现堆栈中的内容匹配了某个产生式的右侧，则将匹配的项从堆栈中弹出，将该产生式左侧的非终结符压入堆栈。这个过程持续进行，直到读取完所有的tokens，并且只有启始非终结符（本例为 program）保留在堆栈中。 产生式右侧的大括号中定义了该规则关联的动作，例如： expr: expr '*' expr { $$ = $1 * $3; } 我们将堆栈中匹配该产生式右侧的项替换为产生式左侧的非终结符，本例中我们弹出 expr '*' expr，然后把 expr 压回堆栈。 我们可以使用 $position 的形式访问堆栈中的项，$1引用的是第一项，$2引用的是第二项，以此类推。$$ 代表的是归约操作执行后的堆栈顶。本例的动作是将三项从堆栈中弹出，两个表达式相加，结果再压回堆栈顶。 上面例子中语法规则关联的动作，在完成语法解析的同时，也完成了表达式求值。一般我们希望语法解析的结果是一棵抽象语法树（AST），可以这么定义语法规则关联的动作： ... %% ... expr: INTEGER { $$ = con($1); } | VARIABLE { $$ = id($1); } | expr '+' expr { $$ = opr('+', 2, $1, $3); } | expr '-' expr { $$ = opr('-', 2, $1, $3); } | expr '*' expr { $$ = opr('*', 2, $1, $3); } | expr '/' expr { $$ = opr('/', 2, $1, $3); } | '(' expr ')' { $$ = $2; } ; %% nodeType *con(int value) { ... } nodeType *id(int i) { ... } nodeType *opr(int oper, int nops, ...) { ... } 上面是一个语法规则定义的片段，我们可以看到，每个规则关联的动作不再是求值，而是调用相应的函数，该函数会返回抽象语法树的节点类型 nodeType，然后将这个节点压回堆栈，解析完成时，我们就得到了一颗由 nodeType 构成的抽象语法树。对这个语法树进行遍历访问，可以生成机器代码，也可以解释执行。 至此，我们大致了解了Lex \u0026 Yacc的原理。其实还有非常多的细节，例如如何消除语法的歧义，但我们的目的是读懂TiDB的代码，掌握这些概念已经够用了。 ","date":"2018-05-09","objectID":"/sql-parser/:1:0","tags":["db"],"title":"TiDB SQL Parser 的实现","uri":"/sql-parser/"},{"categories":["tech"],"content":"goyacc 简介 goyacc 是golang版的 Yacc。和 Yacc的功能一样，goyacc 根据输入的语法规则文件，生成该语法规则的go语言版解析器。goyacc 生成的解析器 yyParse 要求词法分析器符合下面的接口： type yyLexer interface { Lex(lval *yySymType) int Error(e string) } 或者 type yyLexerEx interface { yyLexer // Hook for recording a reduction. Reduced(rule, state int, lval *yySymType) (stop bool) // Client should copy *lval. } TiDB没有使用类似 Lex 的工具生成词法分析器，而是纯手工打造，词法分析器对应的代码是 parser/lexer.go， 它实现了 goyacc 要求的接口： ... // Scanner implements the yyLexer interface. type Scanner struct { r reader buf bytes.Buffer errs []error stmtStartPos int // For scanning such kind of comment: /*! MySQL-specific code */ or /*+ optimizer hint */ specialComment specialCommentScanner sqlMode mysql.SQLMode } // Lex returns a token and store the token value in v. // Scanner satisfies yyLexer interface. // 0 and invalid are special token id this function would return: // return 0 tells parser that scanner meets EOF, // return invalid tells parser that scanner meets illegal character. func (s *Scanner) Lex(v *yySymType) int { tok, pos, lit := s.scan() v.offset = pos.Offset v.ident = lit ... } // Errors returns the errors during a scan. func (s *Scanner) Errors() []error { return s.errs } 另外lexer 使用了字典树技术进行 token 识别，具体的实现代码在parser/misc.go ","date":"2018-05-09","objectID":"/sql-parser/:2:0","tags":["db"],"title":"TiDB SQL Parser 的实现","uri":"/sql-parser/"},{"categories":["tech"],"content":"TiDB SQL Parser的实现 终于到了正题。有了上面的背景知识，对TiDB 的 SQL Parser 模块会相对容易理解一些。先看SQL语法规则文件parser.y，goyacc 就是根据这个文件生成SQL语法解析器的。 parser.y 有6500多行，第一次打开可能会被吓到，其实这个文件仍然符合我们上面介绍过的结构： ... definitions ... %% ... rules ... %% ... subroutines ... parser.y 第三部分 subroutines 是空白没有内容的， 所以我们只需要关注第一部分 definitions 和第二部分 rules。 第一部分主要是定义token的类型、优先级、结合性等。注意 union 这个联合体结构体： %union { offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 该联合体结构体定义了在语法解析过程中被压入堆栈的项的属性和类型。 压入堆栈的项可能是终结符，也就是 token，它的类型可以是item 或 ident； 这个项也可能是非终结符，即产生式的左侧，它的类型可以是 expr 、 statement 、 item 或 ident。 goyacc 根据这个 union 在解析器里生成对应的 struct 是： type yySymType struct { yys int offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 在语法解析过程中，非终结符会被构造成抽象语法树（AST）的节点 ast.ExprNode 或 ast.StmtNode。抽象语法树相关的数据结构都定义在 ast 包中，它们大都实现了 ast.Node 接口： // Node is the basic element of the AST. // Interfaces embed Node should have 'Node' name suffix. type Node interface { Accept(v Visitor) (node Node, ok bool) Text() string SetText(text string) } 这个接口有一个 Accept 方法，接受 Visitor 参数，后续对 AST 的处理，主要依赖这个 Accept 方法，以 Visitor 模式遍历所有的节点以及对 AST 做结构转换。 // Visitor visits a Node. type Visitor interface { Enter(n Node) (node Node, skipChildren bool) Leave(n Node) (node Node, ok bool) } 例如 plan.preprocess 是对 AST 做预处理，包括合法性检查以及名字绑定。 union 后面是对 token 和 非终结符 按照类型分别定义： /* 这部分的token是 ident类型 */ %token \u003cident\u003e ... add \"ADD\" all \"ALL\" alter \"ALTER\" analyze \"ANALYZE\" and \"AND\" as \"AS\" asc \"ASC\" between \"BETWEEN\" bigIntType \"BIGINT\" ... /* 这部分的token是 item 类型 */ %token \u003citem\u003e /*yy:token \"1.%d\" */ floatLit \"floating-point literal\" /*yy:token \"1.%d\" */ decLit \"decimal literal\" /*yy:token \"%d\" */ intLit \"integer literal\" /*yy:token \"%x\" */ hexLit \"hexadecimal literal\" /*yy:token \"%b\" */ bitLit \"bit literal\" andnot \"\u0026^\" assignmentEq \":=\" eq \"=\" ge \"\u003e=\" ... /* 非终结符按照类型分别定义 */ %type \u003cexpr\u003e Expression \"expression\" BoolPri \"boolean primary expression\" ExprOrDefault \"expression or default\" PredicateExpr \"Predicate expression factor\" SetExpr \"Set variable statement value's expression\" ... %type \u003cstatement\u003e AdminStmt \"Check table statement or show ddl statement\" AlterTableStmt \"Alter table statement\" AlterUserStmt \"Alter user statement\" AnalyzeTableStmt \"Analyze table statement\" BeginTransactionStmt \"BEGIN TRANSACTION statement\" BinlogStmt \"Binlog base64 statement\" ... %type \u003citem\u003e AlterTableOptionListOpt \"alter table option list opt\" AlterTableSpec \"Alter table specification\" AlterTableSpecList \"Alter table specification list\" AnyOrAll \"Any or All for subquery\" Assignment \"assignment\" ... %type \u003cident\u003e KeyOrIndex \"{KEY|INDEX}\" ColumnKeywordOpt \"Column keyword or empty\" PrimaryOpt \"Optional primary keyword\" NowSym \"CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP\" NowSymFunc \"CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP/NOW\" ... 第一部分的最后是对优先级和结合性的定义： ... %precedence sqlCache sqlNoCache %precedence lowerThanIntervalKeyword %precedence interval %precedence lowerThanStringLitToken %precedence stringLit ... %right assignmentEq %left pipes or pipesAsOr %left xor %left andand and %left between ... parser.y文件的第二部分是SQL语法的产生式和每个规则对应的 aciton 。SQL语法非常复杂，parser.y 的大部分内容都是产生式的定义。 SQL 语法可以参照MySQL参考手册的SQL Statement Syntax 部分，例如 SELECT 语法的定义如下： SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [PARTITION partition_list] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [INTO OUTFILE 'file_name' [CHARACTER SET charset_name] export_options | INTO DUMPFILE 'file_name' | INTO var_name [, var_name]] [FOR UPDATE | LOCK IN SHARE MODE]] 我们可以在 parser.y 中找到 SELECT 语句的产生式： SelectStmt: \"SELECT\" SelectStmtOpts","date":"2018-05-09","objectID":"/sql-parser/:3:0","tags":["db"],"title":"TiDB SQL Parser 的实现","uri":"/sql-parser/"},{"categories":["tech"],"content":"配合这篇《基于代价的优化》 阅读。 CBO的整体思路是：从逻辑查询计划树，自上而下枚举每个逻辑运算符可能的物理算子，从所有可能的执行路径中选择一条评估代价最小的作为物理查询计划。 一个逻辑运算符受两个因素的影响，导致生成多个候选的物理执行计划： 逻辑运算符可能有多种候选的物理算子供选择，如下表： 有些物理算子会根据参与运算的属性、属性的顺序等因素，生成多种物理执行计划，例如Join的物理算子会根据参与连接的表的顺序，生成多种可能的执行计划。 CBO核心流程的代码在plan/optimizer.go中的physicalOptimize： func physicalOptimize(logic LogicalPlan) (PhysicalPlan, error) { logic.preparePossibleProperties() _, err := logic.deriveStats() if err != nil { return nil, errors.Trace(err) } t, err := logic.findBestTask(\u0026requiredProp{taskTp: rootTaskType, expectedCnt: math.MaxFloat64}) if err != nil { return nil, errors.Trace(err) } p := t.plan() p.ResolveIndices() return p, nil } 三行关键的代码： logic.preparePossibleProperties()：裁剪参与运算的属性，从而尽可能早的裁减掉成物理计划搜索路径上的分支 logic.deriveStats()：为每个逻辑计划节点生成统计信息，为评估物理计划的代价做准备 ![ logic.findBestTask：生成执行代价最小的task findBestTask的核心逻辑： for _, pp := range p.self.exhaustPhysicalPlans(prop) { // find best child tasks firstly. childTasks = childTasks[:0] for i, child := range p.children { childTask, err := child.findBestTask(pp.getChildReqProps(i)) if err != nil { return nil, errors.Trace(err) } childTasks = append(childTasks, childTask) } // combine best child tasks with parent physical plan. curTask := pp.attach2Task(childTasks...) // get the most efficient one. if curTask.cost() \u003c bestTask.cost() { bestTask = curTask } } 首先枚举可能的物理执行计划p.self.exhaustPhysicalPlans，然后遍历每种候选计划，找到代价最小的task。这是个递归的过程，当前节点的代价是由所有子节点的代价组成的，所以在遍历的过程中，又会调用 child.findBestTask(pp.getChildReqProps(i))找到子节点的最佳task。 如何评估物理执行计划的代价呢？根据参与运算的关系（表）的统计信息进行评估。代价评估相关逻辑涉及的代码： 计算关系的统计信息：plan/stats.go 计算task的代价：plan/task.go中的attach2Task系列方法。 ","date":"2018-05-08","objectID":"/cbo-guide/:0:0","tags":["db"],"title":"基于代价优化（CBO）实现代码导读","uri":"/cbo-guide/"},{"categories":["tech"],"content":"Go的错误处理机制很简洁，使用errors.New(text)创建 error，方法的调用者一般按照如下模式处理： if err != nil { return err } 这样做最大的问题是error中没有保存方法调用栈等上下文信息，只能靠创建时传递的string参数来区分error，很难定位错误发生的具体位置。例如： import ( \"fmt\" \"errors\" ) func f1() error { return f2() } func f2() error { return f3() } func f3() error { return errors.New(\"std error\") } func main() { if err := f1(); err != nil { fmt.Printf(\"%+v\", err) } } 执行的输出为： std error 在实际的程序中调用关系复杂，仅凭错误信息很难定位错误源头。TiDB 使用了juju/errors来记录调用栈： import ( \"github.com/juju/errors\" \"fmt\" ) func jf1() error { err := jf2() if err != nil { return errors.Trace(err) } return nil } func jf2() error { err := jf3() if err != nil { return errors.Trace(err) } return nil } func jf3() error { return errors.New(\"juju error\") } func main() { if err := jf1(); err != nil { fmt.Printf(\"%+v\", err) } } 这段代码的输出为： github.com/mz1999/error/main.go:25: juju error github.com/mz1999/error/main.go:19: github.com/mz1999/error/main.go:11: 可以看到，如果想记录调用栈，每次都需要调用errors.Trace。这样做比较繁琐，而且每次trace时内部都会调用runtime.Caller，性能不佳。TiDB已经调研了新的第三方包pkg/errors准备替换掉juju/errors。 使用pkg/errors会简单很多，和标准库的errors一致，但可以记录调用栈信息： import ( \"fmt\" \"github.com/pkg/errors\" ) func pf1() error { return pf2() } func pf2() error { return pf3() } func pf3() error { return errors.New(\"pkg error\") } func main() { if err := pf1(); err != nil { fmt.Printf(\"%+v\", err) } } 这段代码的输出为： pkg error main.pf3 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:17 main.pf2 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:13 main.pf1 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:9 main.main /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:21 runtime.main /usr/local/go/src/runtime/proc.go:198 runtime.goexit /usr/local/go/src/runtime/asm_amd64.s:2361 这样看，使用pkg/errors替代标准库的errors就可以满足我们的需求。 另外，pkg/errors的作者还给了一些最佳实践的建议： 在你自己的代码中，在错误的发生点使用errors.New 或 errors.Errorf ： func parseArgs(args []string) error { if len(args) \u003c 3 { return errors.Errorf(\"not enough arguments, expected at least 3, got %d\", len(args)) } // ... } 如果你接收到一个error，一般简单的直接返回： if err != nil { return err } 如果你是调用第三方的包或标准库时接收到error，使用 errors.Wrap or errors.Wrapf 包装这个error，它会记录在这个点的调用栈： f, err := os.Open(path) if err != nil { return errors.Wrapf(err, \"failed to open %q\", path) } Always return errors to their caller rather than logging them throughout your program. 在程序的top level，或者是worker goroutine，使用 %+v 输出error的详细信息。 func main() { err := app.Run() if err != nil { fmt.Printf(\"FATAL: %+v\\n\", err) os.Exit(1) } } 如果需要抛出包含MySQL错误码的内部错误，可以使用errors.Wrap包装，附带上报错位置的调用栈信息： func pf3() error { return errors.Wrap(mysql.NewErr(mysql.ErrCantCreateTable, \"tablename\", 500), \"\") } 这样，我们既拿到了完整调用栈，又可以使用errors.Cause获取MySQL的错误码等信息： if err := pf1(); err != nil { fmt.Printf(\"%+v\", err) var sqlError *mysql.SQLError if m, ok := errors.Cause(err).(*mysql.SQLError); ok { sqlError = m } else { sqlError = mysql.NewErrf(mysql.ErrUnknown, \"%s\", err.Error()) } fmt.Printf(\"\\nMySQL error code: %d, state: %s\", sqlError.Code, sqlError.State) } ","date":"2018-04-06","objectID":"/golang-error-handling/:0:0","tags":["golang"],"title":"Golang error处理实践","uri":"/golang-error-handling/"},{"categories":["tech"],"content":"Go中的引用类型不是指针，而是对指针的包装，在它的内部通过指针引用底层数据结构。每一种引用类型也包含一些其他的field，用来管理底层的数据结构。 看一个例子比较直观： s := []string{\"a\",\"b\", \"c\"} fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) 简单解释一下这段代码。先初始化一个slice，然后使用unsafe.Pointer(\u0026s)把slice的指针转换为通用指针Pointer。Pointer是可以代表任何数据类型的指针。最后把Pointer强制转换为*reflect.SliceHeader。SliceHeader代表的是slice运行时数据结构，定义如下： type SliceHeader struct { Data uintptr Len int Cap int } 可以看到，SliceHeader内部有一个用来指向底层数组的指针Data，另外还有两个属性Len和Cap用来保存slice的内部状态。 上面的代码运行结果如下： \u0026reflect.SliceHeader{Data:0xc420078180, Len:3, Cap:3} slice可以自动扩容，当底层数组容量不够时，会自动创建一个新的数组替换。让我们做个实验： s := []string{\"a\",\"b\", \"c\"} fmt.Printf(\"%#v \\n\", s) fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) fmt.Println() s = append(s, \"d\") fmt.Printf(\"%#v \\n\", s) fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) fmt.Println() s = append(s, \"e\") fmt.Printf(\"%#v \\n\", s) fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) fmt.Println() 运行结果如下： 对于初始化容量为3的slice，在向这个slice append 新元素时，底层会创建一个容量翻倍的新数组，并将原先的内容复制过来，再将新元素append到最后。我们可以看到这个slice内部保存底层数组的指针在第一次append后，指向了新的地址。当再向它append新元素时，由于底层数组还有空间，内部指针保持不变，只是更新Len属性为5。 在Go中进行函数调用时，参数都是按值传递的。对于引用类型也是按值传递，会复制引用本身，但不会复制引用指向的底层数据结构。还是看代码： func foo(s []string) { fmt.Println(\"======= func foo =======\") s = append(s, \"f\") fmt.Printf(\"%#v \\n\", s) fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) fmt.Printf(\"\u0026s: %p \\n\", \u0026s) fmt.Println(\"========================\\n\") } func main() { ...... s = append(s, \"e\") fmt.Printf(\"%#v \\n\", s) fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) fmt.Printf(\"\u0026s: %p \\n\", \u0026s) fmt.Println() foo(s) fmt.Printf(\"%#v \\n\", s) fmt.Printf(\"%#v \\n\", (*reflect.SliceHeader)(unsafe.Pointer(\u0026s))) fmt.Printf(\"\u0026s: %p \\n\", \u0026s) } 运行结果为： 在函数调用时，传递给函数的slice进行了复制，函数的参数是一个新的slice，但slice内部指针指向的底层数组还是同一个。 完整的示例代码在https://play.golang.org/p/qwwSuskLfCa，可以在Playground中直接运行。 Go语言的引用类型有slice, map, channel, interface和function。技术上，string也是引用类型： type StringHeader struct { Data uintptr Len int } 有时候为了性能优化，可以利用[]byte和string头部结构的“部分相同”，以非安全的指针类型转换来实现类型变更，避免底层数组的复制。例如 TiDB 中就使用了这个技巧： // String converts slice to string without copy. // Use at your own risk. func String(b []byte) (s string) { if len(b) == 0 { return \"\" } pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026s)) pstring.Data = pbytes.Data pstring.Len = pbytes.Len return } // Slice converts string to slice without copy. // Use at your own risk. func Slice(s string) (b []byte) { pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026s)) pbytes.Data = pstring.Data pbytes.Len = pstring.Len pbytes.Cap = pstring.Len return } 上面两个函数实现了[]byte和string的互相转换，不需要底层数组的copy。 ","date":"2018-03-05","objectID":"/go-reference-types/:0:0","tags":["golang"],"title":"Go语言的引用类型","uri":"/go-reference-types/"},{"categories":["tech"],"content":"TiDB提供了docker compose的部署方式，可以很方便的在单机上搭建一个TiDB集群作为开发测试环境。如果修改了TiDB源码，可以使用这样方式，先在本机部署集群做一些验证。 首先本机要安装docker和docker compose，建议参考官方文档Install Docker 和 Install Docker Compose 下载tidb-docker-compose项目 git clone https://github.com/pingcap/tidb-docker-compose.git 使用docker compose启动TiDB集群 cd tidb-docker-compose \u0026\u0026 sudo docker-compose up -d 就这么简单，集群启动成功了。使用docker ps查看： 可以看到，已经启动了三个tikv实例，一个tidb实例，三个pd实例，还有监控和tidb-vision。 监控的访问地址是 http://localhost:3000，用户名/密码：admin/admin。 tidb-vision 的访问地址是 http://localhost:8010 使用MySQL客户端访问TiDB 如果本机有MySQL客户端，可以直接连接： mysql -h 127.0.0.1 -P 4000 -u root 如果本机没有MySQL客户端，可以使用docker启动一个MySQL容器，然后登录到容器内，再使用MySQL客户端连接TiDB集群。这种方式比较环保，只要有docker环境就行。先查看TiDB集群的docker网络： 然后启动MySQL容器，注意要加入TiDB集群的docker网络： sudo docker run --network=tidbdockercompose_default --rm -it mysql /bin/bash 因为和TiDB集群在同一个docker网络，在MySQL容器内，可以使用tidb名称访问到TiDB： mysql -h tidb -P 4000 -u root 停止集群 sudo docker-compose down 如果自己build了TiDB版本想在本机run集群，文档写的很清楚，告诉你镜像应该放在什么位置。 Have fun! ","date":"2018-02-09","objectID":"/tidb-docker-compose/:0:0","tags":["db"],"title":"利用docker compose在单机上玩转TiDB","uri":"/tidb-docker-compose/"},{"categories":["tech"],"content":"翻了一下TiDB的文档，对TiDB有了个大概的了解。简单说，TiDB的实现架构是：底层是分布式KV引擎TiKV，上层是SQL引擎TiDB Servers。一般传统数据库也是这么分层实现的，只不过TiKV实现了一个分布式、强一致、支持事务的K/V，不像数据库是单机版K/V。在TiKV之上实现SQL引擎就简化了很多，因此TiDB Servers是无状态的。 简化的抽象架构分层： TiDB官方文档里的架构图： 可以看出，TiDB的基础工作和最突出的创新在TiKV，理论上有了这个KV，可以把单机版的SQl引擎实现方式搬过来，就有了一个可扩展的分布式数据库。 那就看看TiKV的架构：用RocksDB作为单机存储引擎，然后上层用Raft实现了一个分布式、强一致性的K/V。有了这个很强大的分布式K/V，在上面实现了MVCC层，就是对每个Key加了version，然后基于MVCC层最终实现了分布式事务。 RocksDB内部用的是LSM-Tree，写入性能肯定比MySQL的B+ tree好。读取性能看实现的优化情况了，不过RocksDB是Facebook做的，应该没啥问题。 Raft的实现和测试用例是从Etcd完全拷贝过来的，可以认为Raft的实现也是稳定的。 作者的原话： 我们做了一件比较疯狂的事情，就是我们把 Etcd 的 Raft 状态机的每一行代码，line by line 的翻译成了 Rust。而我们第一个转的就是所有 Etcd 本身的测试用例。我们写一模一样的 test ，保证这个东西我们 port 的过程是没有问题的。 分布式事务参照的是Percolator。Percolator和Spanner差不多，只不过Spanner引入了专有硬件原子钟，而Percolator依靠单点的授时服务器。两者都是对两阶段提交协议的改进。我们搞过J2EE，对两阶段提交协议应该比较熟悉，2PC的问题是：一旦事务参与者投票，它必须等待coordinator给出指示：提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery（HeuristicCommitException、HeuristicMixedException、HeuristicRollbackException等异常就是遇到了这种情况，只好资源自己做了决定）。这么看在本质上，2PC为了达到一致性，实际上是退化到由coordinator单节点来实现atomic commit. Spanner引入了trueTime api，底下存储是MVCC，每行数据都带一个时间戳做version，TrueTime API就是打时间戳的，用时间戳标识事务顺序，解决2PC依赖单点coordinator的问题。而依赖单点的授时服务器的问题，他们是这样解释的： 因为 TSO 的逻辑极其简单，只需要保证对于每一个请求返回单调递增的 id 即可，通过一些简单的优化手段（比如 pipeline）性能可以达到每秒生成百万 id 以上，同时 TSO 本身的高可用方案也非常好做，所以整个 Percolator 模型的分布式程度很高。 TiDB的事务隔离级别实现了Read committed和Repeatable read，没有实现最严格的Serializable。不过串行化的隔离级别在现实中很少使用，性能会很差。oracle 11g也没有实现它。oracle实现的是snapshot isolation，实际上比串行化的保证要弱。TiDB和oracle都用是MVCC保证了Repeatable read，简单说就是每个事务都读取一个一致性的snapshot，这个snapshot肯定就是完整状态。所以叫做snapshot isolation。按照TiDB的文档，TiDB 实现的 snapshot 隔离级别，该隔离级别不会出现幻读，但是会出现写偏斜。 写偏斜是什么，举个简单的例子：两个事务都先分别查询在线值班的医生总数，发现还有两个在线的医生，然后各自更新不同的记录，分别让不同的医生下线。事务提交后，两个医生都下线了，没有一个医生在线值班，出现错误的业务场景。这种异常情况是两个事务分别更新不同的记录。引起写倾斜的的模式：先查询很多列看是否满足某种条件，然后依赖查询结果写入数据并提交。解决的方法有：真正的串行化隔离级别，或者显示的锁定事务依赖的行。 从文档看，TiDB利用了成熟的开源项目，自己实现了分布式事务、分布式存储和SQL引擎，整体方案诱人，至于软件成熟程度，还需要经过实际的使用测试。 ","date":"2018-02-09","objectID":"/tidb-glance/:0:0","tags":["db"],"title":"TiDB初探","uri":"/tidb-glance/"},{"categories":["tech"],"content":"为Wireshark编写HSF2协议解析插件 Wireshark是排查网络问题最常用的工具，它已经内置支持了上百种通用协议，同时它的扩展性也很好，对于自定义的应用层网络协议，你可以使用c或者lua编写协议解析插件，这样你就可以在Wireshark中观察到协议的内容而不是二进制流，为排查问题带来一定的便利性。 最近在排查一个HSF超时的问题，顺便花了些时间为Wireshark写了一个HSF2协议解析插件，目前支持HSF2的request、response和heart beat协议，支持将多个packet还原为上层PDU。暂不支持HSF原先的TB Remoting协议。先看效果。 首先在Packet List区域已经能识别HSF2协议： HSF的请求和响应 HSF的心跳协议 点击某个数据包，可以在Packet details区域查看详细的协议内容： HSF请求 可以看到很多协议的重要信息，包括序列化方式，超时时间，服务名称、方法及参数 HSF响应 HeartBeat请求 心跳协议比较简单，响应就不看了。 插件是使用lua开发的，安装比较简单，以OS X平台为例： 将协议解析脚本copy到/Applications/Wireshark.app/Contents/Resources/share/wireshark/ 目录 编辑init.lua文件，设置disable_lua = false，确保lua支持打开 在init.lua文件末尾增加 dofile(\"hsf2.lua\") 再次启动Wireshark，会对12200端口的数据流使用脚本解析，已经可以识别HSF协议了。 备注 附上hsf2.lua，边翻HSF代码边写的，写完眼已经花了，错误难免，欢迎试用。 -- declare the protocol hsf2_proto = Proto(\"hsf2\", \"Taobao HSF2 Protocol\") -- declare the value strings local vs_id = { [12] = \"HSF2 Heart Beat\", [13] = \"HSF2 TB Remoting\", [14] = \"HSF2 HSF Remoting\" } local vs_version = { [1] = \"HSF2\" } local vs_op = { [0] = \"request\", [1] = \"response\" } local vs_codectype = { [1] = \"HESSIAN_CODEC\", [2] = \"JAVA_CODEC\", [3] = \"TOP_CODEC\", [4] = \"HESSIAN2_CODEC\", [5] = \"KRYO_CODEC\", [6] = \"JSON_CODEC\", [7] = \"CUSTOMIZED_CODEC\", } local vs_responsestatus = { [20] = \"OK\", [30] = \"client timeout\", [31] = \"server timeout\", [40] = \"bad request\", [50] = \"bad response\", [60] = \"service not found\", [70] = \"service error\", [80] = \"server error\", [90] = \"client error\", [91] = \"Unknow error\", [81] = \"Thread pool is busy\", [82] = \"Communication error\", [88] = \"server will close soon\", [10] = \"server send coders\", [83] = \"Unkown code\" } -- declare the fields local f_id = ProtoField.uint8(\"hsf2.id\", \"Identification\", base.Dec, vs_id) local f_version = ProtoField.uint8(\"hsf2.version\", \"version\", base.Dec, vs_version) local f_op = ProtoField.uint8(\"hsf2.op\", \"operation\", base.DEC, vs_op) local f_codectype = ProtoField.uint8(\"hsf2.codectype\", \"codectype\", base.DEC, vs_codectype) local f_reserved = ProtoField.uint8(\"hsf2.reserved\", \"reserved\", base.DEC) local f_req_id = ProtoField.uint64(\"hsf2.req_id\", \"RequestID\", base.DEC) local f_timeout = ProtoField.uint32(\"hsf2.timeout\", \"timeout\", base.DEC) local f_service_name_len = ProtoField.uint32(\"hsf2.service_name_len\", \"Service Name length\", base.DEC) local f_method_name_len = ProtoField.uint32(\"hsf2.method_name_len\", \"Method Name length\", base.DEC) local f_arg_count = ProtoField.uint32(\"hsf2.arg.count\", \"Argument Count\", base.DEC) local f_arg_type_len = ProtoField.uint32(\"hsf2.arg.type.len\", \"Argument Type length\", base.DEC) local f_arg_obj_len = ProtoField.uint32(\"hsf2.arg.obj.len\", \"Argument Object length\", base.DEC) local f_req_prop_len = ProtoField.uint32(\"hsf2.req.prop.len\", \"Request Prop Length\", base.DEC) local f_service_name = ProtoField.string(\"hsf2.service.name\", \"Service Name\") local f_method_name = ProtoField.string(\"hsf2.method.name\", \"Method Name\") local f_arg_type = ProtoField.string(\"hsf2.arg.type\", \"Argument Type\") local f_arg_obj = ProtoField.bytes(\"hsf2.arg.obj\", \"Argument Object\") local f_req_prop = ProtoField.bytes(\"hsf2.req.prop\", \"Request Prop\") local f_response_status = ProtoField.uint32(\"hsf2.response.status\", \"Response Status\", base.DEC, vs_responsestatus) local f_response_body_len = ProtoField.uint32(\"hsf2.response.body.len\", \"Response Body Length\", base.DEC) local f_response_body = ProtoField.bytes(\"hsf2.response.body\", \"Response Body\", base.DEC) hsf2_proto.fields = { f_id, f_version, f_op, f_codectype, f_reserved, f_req_id, f_timeout, f_service_name_len, f_method_name_len, f_arg_count, f_arg_type_len, f_arg_obj_len, f_req_prop_len, f_service_name, f_method_name, f_arg_type, f_arg_obj, f_req_prop, f_response_status, f_response_body_len, f_response_body } function get_pdu_length(buffer) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 -- heart beat if id == 12 then return 18 end -- TB REMOTING if id == 13 then -- TODO return 18 end -- HSF REMOTING if id == 14 then local version = buffer(offs","date":"2014-12-21","objectID":"/hsf/:0:0","tags":["java","networking"],"title":"为Wireshark编写HSF2协议解析插件","uri":"/hsf/"},{"categories":["tech"],"content":"DNS（Domain Name System）简单说就是一个名称到IP地址的映射，使用容易记住的域名代替IP地址。基本原理就不讲了，网上的文章很多。 了解了基本原理，你就可以使用dig(Domain Information Groper）命令进行探索。当对淘宝的几个域名进行dig时，你发现事情并不像想像的那么简单。 为了使大家的输出一致，我在dig命令中显示的指定了DNS服务器（@222.172.200.68 云南电信DNS）。 dig @222.172.200.68 login.alibaba-inc.com dig @222.172.200.68 guang.taobao.com dig @222.172.200.68 item.taobao.com 你会发现对于域名的查询，都不是直接返回IP地址这么简单，而是经过了神奇的CNAME。一般文档在介绍CNAME时只是说可以给一个域名指定别名（alias），其实这是DNS运维非常重要的手段，使得DNS配置具有一定的灵活性和可扩展性。结合上面三个域名的解析说一下。先给一张高大上的图，是按照我自己的理解画的，不一定完全正确:) 图上分了三层，最上层是常规的DNS解析过程，用户通过local DNS做递归查询，最终定位到taobao.com权威DNS服务器。 中间层可以称为GSLB（Global Server Load Balancing），作用是提供域名的智能解析，根据一定的策略返回结果。淘系目前有三套GSLB： F5 GTM：F5的硬件设备，基本已经被淘汰，全部替换为自研软件。GTM功能强大，但对用户而言是黑盒，性能一般价格昂贵。早期淘宝CDN智能调度就是基于F5 GTM做的。 ADNS：阿里自研权威DNS，替换GTM。ADNS很牛逼，可惜资料太少。 Pharos：阿里CDN的大脑，实现CDN流量精确，稳定，安全的调度。 taobao.com权威DNS服务器会根据不用的域名，CNAME到不同的GSLB做智能调度。CNAME的作用有点类似请求分发，taobao.com权威DNS服务器将域名解析请求转交给下一层域名服务器处理。 最下层是应用层，提供真正的服务。 现在再看看这三个域名的解析。 login.alibaba-inc.com 被转交给了GTM做智能解析，GTM通过返回不同机房的VIP做流量调度，用户的请求最终经过LVS到达我们的应用。 guang.taobao.com的解析过程和login.alibaba-inc.com类似，只不过智能调度换成了ADNS。 item.taobao.com有点小复杂。我们都知道CDN是做静态资源加速的，像这样的静态资源域名img04.taobaocdn.com会由Pharos解析调度，为用户返回就近的CDN节点。但什么时候动态内容也经过CDN代理了？这就是高大上的统一接入层。简单说下过程：item.taobao.com通过Pharos的智能调度，返回给用户就近的CDN节点。当用户的请求到达CDN节点时，这个节点会为动态内容的域名选择合适的后端服务，相当于每次都做回源处理。这个CDN节点可以理解为用户请求的代理。CDN在选择后端服务时，会执行单元化、小淘宝等逻辑，将请求发送到正确的机房。请求到达机房后，先进入统一接入层，注意这里的后端应用不需要申请VIP，IP地址列表保存在VIPServer中。统一接入层从VIPServer中拿到后端应用的IP地址列表，进行请求分发。VIPServer的作用类似HSF的ConfigServer，可以大大减少应用VIP的数量。以后做单元化部署的域名都会接入统一接入层，将单元化的逻辑上推到了CDN节点。 貌似是讲清楚了，不过这个过程已经做了很大的简化，因为我也仅仅是了解个大概。作为业务开发重点关注的是最下面的Java应用，转岗到技术保障后，才发现有机会可以从全局了解网站架构，接触到网络、DNS、CDN、LVS\u0026VIP等等基础设施。 ","date":"2014-09-10","objectID":"/dns/:0:0","tags":["linux","networking"],"title":"从开发角度看DNS","uri":"/dns/"},{"categories":["tech"],"content":"在Linux上做网络应用的性能优化时，一般都会对TCP相关的内核参数进行调节，特别是和缓冲、队列有关的参数。网上搜到的文章会告诉你需要修改哪些参数，但我们经常是知其然而不知其所以然，每次照抄过来后，可能很快就忘记或混淆了它们的含义。本文尝试总结TCP队列缓冲相关的内核参数，从协议栈的角度梳理它们，希望可以更容易的理解和记忆。注意，本文内容均来源于参考文档，没有去读相关的内核源码做验证，不能保证内容严谨正确。作为Java程序员没读过内核源码是硬伤。 下面我以server端为视角，从连接建立、数据包接收和数据包发送这3条路径对参数进行归类梳理。 ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:0","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"一、连接建立 简单看下连接的建立过程，客户端向server发送SYN包，server回复SYN＋ACK，同时将这个处于SYN_RECV状态的连接保存到半连接队列。客户端返回ACK包完成三次握手，server将ESTABLISHED状态的连接移入accept队列，等待应用调用accept()。 可以看到建立连接涉及两个队列： 半连接队列，保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置 accept队列，保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn, backlog)。其中backlog是我们创建ServerSocket(int port,int backlog)时指定的参数，最终会传递给listen方法： #include \u003csys/socket.h\u003e int listen(int sockfd, int backlog); 如果我们设置的backlog大于net.core.somaxconn，accept队列的长度将被设置为net.core.somaxconn 另外，为了应对SYN flooding（即客户端只发送SYN包发起握手而不回应ACK完成连接建立，填满server端的半连接队列，让它无法处理正常的握手请求），Linux实现了一种称为SYN cookie的机制，通过net.ipv4.tcp_syncookies控制，设置为1表示开启。简单说SYN cookie就是将连接信息编码在ISN(initial sequence number)中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。对于一去不复返的客户端握手，不理它就是了。 ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:1","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"二、数据包的接收 先看看接收数据包经过的路径： 数据包的接收，从下往上经过了三层：网卡驱动、系统内核空间，最后到用户态空间的应用。Linux内核使用sk_buff(socket kernel buffers)数据结构描述一个数据包。当一个新的数据包到达，NIC（network interface controller）调用DMA engine，通过Ring Buffer将数据包放置到内核内存区。Ring Buffer的大小固定，它不包含实际的数据包，而是包含了指向sk_buff的描述符。当Ring Buffer满的时候，新来的数据包将给丢弃。一旦数据包被成功接收，NIC发起中断，由内核的中断处理程序将数据包传递给IP层。经过IP层的处理，数据包被放入队列等待TCP层处理。每个数据包经过TCP层一系列复杂的步骤，更新TCP状态机，最终到达recv Buffer，等待被应用接收处理。有一点需要注意，数据包到达recv Buffer，TCP就会回ACK确认，既TCP的ACK表示数据包已经被操作系统内核收到，但并不确保应用层一定收到数据（例如这个时候系统crash），因此一般建议应用协议层也要设计自己的ACK确认机制。 上面就是一个相当简化的数据包接收流程，让我们逐层看看队列缓冲有关的参数。 网卡Bonding模式 当主机有1个以上的网卡时，Linux会将多个网卡绑定为一个虚拟的bonded网络接口，对TCP/IP而言只存在一个bonded网卡。多网卡绑定一方面能够提高网络吞吐量，另一方面也可以增强网络高可用。Linux支持7种Bonding模式： - `Mode 0 (balance-rr)` Round-robin策略，这个模式具备负载均衡和容错能力 - `Mode 1 (active-backup)` 主备策略，在绑定中只有一个网卡被激活，其他处于备份状态 - `Mode 2 (balance-xor)` XOR策略，通过源MAC地址与目的MAC地址做异或操作选择slave网卡 - `Mode 3 (broadcast)` 广播，在所有的网卡上传送所有的报文 - `Mode 4 (802.3ad)` IEEE 802.3ad 动态链路聚合。创建共享相同的速率和双工模式的聚合组 - `Mode 5 (balance-tlb)` Adaptive transmit load balancing - `Mode 6 (balance-alb)` Adaptive load balancing 详细的说明参考内核文档Linux Ethernet Bonding Driver HOWTO。我们可以通过cat /proc/net/bonding/bond0查看本机的Bonding模式： 一般很少需要开发去设置网卡Bonding模式，自己实验的话可以参考这篇文档 网卡多队列及中断绑定 随着网络的带宽的不断提升，单核CPU已经不能满足网卡的需求，这时通过多队列网卡驱动的支持，可以将每个队列通过中断绑定到不同的CPU核上，充分利用多核提升数据包的处理能力。 首先查看网卡是否支持多队列，使用lspci -vvv命令，找到Ethernet controller项： 如果有MSI-X， Enable+ 并且Count \u003e 1，则该网卡是多队列网卡。 然后查看是否打开了网卡多队列。使用命令cat /proc/interrupts，如果看到eth0-TxRx-0表明多队列支持已经打开： 最后确认每个队列是否绑定到不同的CPU。cat /proc/interrupts查询到每个队列的中断号，对应的文件/proc/irq/${IRQ_NUM}/smp_affinity为中断号IRQ_NUM绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核： ``` （00000001）代表CPU0 （00000010）代表CPU1 （00000011）代表CPU0和CPU1 ``` 如果绑定的不均衡，可以手工设置，例如： ``` echo \"1\" \u003e /proc/irq/99/smp_affinity echo \"2\" \u003e /proc/irq/100/smp_affinity echo \"4\" \u003e /proc/irq/101/smp_affinity echo \"8\" \u003e /proc/irq/102/smp_affinity echo \"10\" \u003e /proc/irq/103/smp_affinity echo \"20\" \u003e /proc/irq/104/smp_affinity echo \"40\" \u003e /proc/irq/105/smp_affinity echo \"80\" \u003e /proc/irq/106/smp_affinity ``` Ring Buffer Ring Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。 可以使用ethtool -g eth0查看当前Ring Buffer的设置： 上面的例子接收队列为4096，传输队列为256。可以通过ifconfig观察接收和传输队列的运行状况： RX errors：收包总的错误数 RX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。 RX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。 当dropped数量持续增加，建议增大Ring Buffer，使用ethtool -G进行设置。 Input Packet Queue(数据包接收队列) 当接收数据包的速率大于内核TCP处理包的速率，数据包将会缓冲在TCP层之前的队列中。接收队列的长度由参数net.core.netdev_max_backlog设置。 recv Buffer recv buffer是调节TCP性能的关键参数。BDP(Bandwidth-delay product，带宽延迟积) 是网络的带宽和与RTT(round trip time)的乘积，BDP的含义是任意时刻处于在途未确认的最大数据量。RTT使用ping命令可以很容易的得到。为了达到最大的吞吐量，recv Buffer的设置应该大于BDP，即recv Buffer \u003e= bandwidth * RTT。假设带宽是100Mbps，RTT是100ms，那么BDP的计算如下： BDP = 100Mbps * 100ms = (100 / 8) * (100 / 1000) = 1.25MB Linux在2.6.17以后增加了recv Buffer自动调节机制，recv buffer的实际大小会自动在最小值和最大值之间浮动，以期找到性能和资源的平衡点，因此大多数情况下不建议将recv buffer手工设置成固定值。 当net.ipv4.tcp_moderate_rcvbuf设置为1时，自动调节机制生效，每个TCP连接的recv Buffer由下面的3元数组指定： net.ipv4.tcp_rmem = \u003cMIN\u003e \u003cDEFAULT\u003e \u003cMAX\u003e 最初recv buffer被设置为，同时这个缺省值会覆盖net.core.rmem_default的设置。随后recv buffer根据实际情况在最大值和最小值之间动态调节。在缓冲的动态调优机制开启的情况下，我们将net.ipv4.tcp_rmem的最大值设置为BDP。 当net.ipv4.tcp_moderate_rcvbuf被设置为0，或者设置了socket选项SO_RCVBUF，缓冲的动态调节机制被关闭。recv buffer的缺省值由net.core.rmem_default设置，但如果设置了net.ipv4.tcp_rmem，缺省值则被\u003cDEFAULT\u003e覆盖。可以通过系统调用setsockopt()设置recv buffer的最大值为net.core.rmem_max。在缓冲动态调节机制关闭的情况下，建议把缓冲的缺省值设置为BDP。 注意这里还有一个细节，缓冲除了保存接收的数据本身，还需要一部分空间保存socket数据结构等额外信息。因此上面讨论的recv buffer最佳值仅仅等于BDP是不够的，还需要考虑保存socket等额外信息的开销。Linux根据参数net.ipv4.tcp_adv_win_scale计算额外开销的大小： Buffer / 2tcp_adv_win_scale 如果net.ipv4.tcp_adv_win_scale的值为1，则二分之一的缓冲空间用来做额外开销，如果为2的话，则四分之一缓冲空间用来做额外开销。因此recv buffer的最佳值应该设置为： BDP / (1 – 1 / 2tcp_adv_win_scale) ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:2","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"三、数据包的发送 发送数据包经过的路径： 和接收数据的路径相反，数据包的发送从上往下也经过了三层：用户态空间的应用、系统内核空间、最后到网卡驱动。应用先将数据写入TCP send buffer，TCP层将send buffer中的数据构建成数据包转交给IP层。IP层会将待发送的数据包放入队列QDisc(queueing discipline)。数据包成功放入QDisc后，指向数据包的描述符sk_buff被放入Ring Buffer输出队列，随后网卡驱动调用DMA engine将数据发送到网络链路上。 同样我们逐层来梳理队列缓冲有关的参数。 send Buffer 同recv Buffer类似，和send Buffer有关的参数如下： net.ipv4.tcp_wmem = \u003cMIN\u003e \u003cDEFAULT\u003e \u003cMAX\u003e net.core.wmem_default net.core.wmem_max 发送端缓冲的自动调节机制很早就已经实现，并且是无条件开启，没有参数去设置。如果指定了tcp_wmem，则net.core.wmem_default被tcp_wmem的覆盖。send Buffer在tcp_wmem的最小值和最大值之间自动调节。如果调用setsockopt()设置了socket选项SO_SNDBUF，将关闭发送端缓冲的自动调节机制，tcp_wmem将被忽略，SO_SNDBUF的最大值由net.core.wmem_max限制。 QDisc QDisc（queueing discipline ）位于IP层和网卡的ring buffer之间。我们已经知道，ring buffer是一个简单的FIFO队列，这种设计使网卡的驱动层保持简单和快速。而QDisc实现了流量管理的高级功能，包括流量分类，优先级和流量整形（rate-shaping）。可以使用tc命令配置QDisc。 QDisc的队列长度由txqueuelen设置，和接收数据包的队列长度由内核参数net.core.netdev_max_backlog控制所不同，txqueuelen是和网卡关联，可以用ifconfig命令查看当前的大小： 使用ifconfig调整txqueuelen的大小： ifconfig eth0 txqueuelen 2000 Ring Buffer 和数据包的接收一样，发送数据包也要经过Ring Buffer，使用ethtool -g eth0查看： 其中TX项是Ring Buffer的传输队列，也就是发送队列的长度。设置也是使用命令ethtool -G。 TCP Segmentation和Checksum Offloading 操作系统可以把一些TCP/IP的功能转交给网卡去完成，特别是Segmentation(分片)和checksum的计算，这样可以节省CPU资源，并且由硬件代替OS执行这些操作会带来性能的提升。 一般以太网的MTU（Maximum Transmission Unit）为1500 bytes，假设应用要发送数据包的大小为7300bytes，MTU1500字节 － IP头部20字节 － TCP头部20字节＝有效负载为1460字节，因此7300字节需要拆分成5个segment： Segmentation(分片)操作可以由操作系统移交给网卡完成，虽然最终线路上仍然是传输5个包，但这样节省了CPU资源并带来性能的提升： 可以使用ethtool -k eth0查看网卡当前的offloading情况： 上面这个例子checksum和tcp segmentation的offloading都是打开的。如果想设置网卡的offloading开关，可以使用ethtool -K(注意K是大写)命令，例如下面的命令关闭了tcp segmentation offload： sudo ethtool -K eth0 tso off 网卡多队列和网卡Bonding模式 在数据包的接收过程中已经介绍过了。 至此，终于梳理完毕。整理TCP队列相关参数的起因是最近在排查一个网络超时问题，原因还没有找到，产生的“副作用”就是这篇文档。再想深入解决这个问题可能需要做TCP协议代码的profile，需要继续学习，希望不久的将来就可以再写文档和大家分享了。 参考文档 Queueing in the Linux Network Stack TCP Implementation in Linux: A Brief Tutorial Impact of Bandwidth Delay Product on TCP Throughput Java程序员也应该知道的系统知识系列之网卡 ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:3","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"Java中通过Socket.setSoLinger设置SO_LINGER选项，有三种组合形式： Socket.setSoLinger(false, linger) 设置为false，这时linger值被忽略。摘自unix network programming： The default action of close with a TCP socket is to mark the socket as closed and return to the process immediately. The socket descriptor is on longer usable by the process: it can’t be used as an argument to read or write. TCP will try to send any data that is already queued to be sent to the other end, and after this occurs, the normal TCP connection termination sequence takes place. 如果设置为false，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入TIME_WAIT状态。 我们可以写个演示程序，客户端发送较大的数据包后，立刻调用close，而server端将Receive Buffer设置的很小。close会立即返回，客户端的Java进程结束，但是当我们用tcpdump/Wireshark抓包会发现，操作系统正在帮你发送数据，内核缓冲区中的数据发送完毕后，发送FIN包关闭连接。 Socket.setSoLinger(true, 0) TCP discards any data still remaining in the socket send buffer and sends an RST to the peer, not the normal four-packet connection termination sequence. 主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送RST，对端会收到java.net.SocketException: Connection reset异常。同样使用tcpdump抓包可以很容易观察到。 另外有些人会用这种方式解决主动关闭放方有大量TIME_WAIT状态连接的问题，因为发送完RST后，连接立即销毁，不会停留在TIME_WAIT状态。一般不建议这么做，除非你有合适的理由： If the a client of your server application misbehaves (times out, returns invalid data, etc.) an abortive close makes sense to avoid being stuck in CLOSE_WAIT or ending up in the TIME_WAIT state. If you must restart your server application which currently has thousands of client connections you might consider setting this socket option to avoid thousands of server sockets in TIME_WAIT (when calling close() from the server end) as this might prevent the server from getting available ports for new client connections after being restarted. On page 202 in the aforementioned book it specifically says: “There are certain circumstances which warrant using this feature to send an abortive close. One example is an RS-232 terminal server, which might hang forever in CLOSE_WAIT trying to deliver data to a stuck terminal port, but would properly reset the stuck port if it got an RST to discard the pending data.” Socket.setSoLinger(true, linger \u003e 0) if there is any data still remaining in the socket send buffer, the process will sleep when calling close() until either all the data is sent and acknowledged by the peer or the configured linger timer expires. if the linger time expires before the remaining data is sent and acknowledged, close returns EWOULDBLOCK and any remaining data in the send buffer is discarded. 如果SO_LINGER选项生效，并且超时设置大于零，调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况： 数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK） 超时，未发送完成的数据被丢弃，连接发送RST进行非正常关闭 类似的我们也可以构造demo观察这种场景。客户端发送较大的数据包，server端将Receive Buffer设置的很小。设置linger为1，调用close时等待1秒。注意SO_LINGER的单位为秒，好多人被坑过。假设close后1秒内缓冲区中的数据发送不完，使用tcpdump/Wireshark可以观察到客户端发送RST包，服务端收到java.net.SocketException: Connection reset异常。 最后，在使用NIO时，最好不设置SO_LINGER，以后会再写一篇文章分析。 ","date":"2014-08-10","objectID":"/so-linger/:0:0","tags":["java","networking"],"title":"TCP `SO_LINGER` 选项对Socket.close的影响","uri":"/so-linger/"},{"categories":["tech"],"content":"早上毕玄转给我一个问题，vsearch在上海机房部署的应用，在应用关闭后，端口释放的时间要比杭州机房的时间长。 TCP的基本知识，主动关闭连接的一方会处于TIME_WAIT状态，并停留两倍的MSL（Maximum segment lifetime）时长。 那就检查一下MSL的设置。网上有很多文章说，可以通过设置net.ipv4.tcp_fin_timeout来控制MSL。其实这有点误导人。查看Linux kernel的文档 ，发现tcp_fin_timeout是指停留在FIN_WAIT_2状态的时间： tcp_fin_timeout - INTEGER The length of time an orphaned (no longer referenced by any application) connection will remain in the FIN_WAIT_2 state before it is aborted at the local end. While a perfectly valid “receive only” state for an un-orphaned connection, an orphaned connection in FIN_WAIT_2 state could otherwise wait forever for the remote to close its end of the connection. Default: 60 seconds 幸好这个问题原先在内部请教过： sysctl调节不了，只能调节复用和回收。 以前改小是改下面文件，重新编译内核的。 grep -i timewait_len /usr/src/kernels/2.6.32-220.el6.x86_64/include/net/tcp.h define TCP_TIMEWAIT_LEN (60HZ) / how long to wait to destroy TIME-WAIT define TCP_FIN_TIMEOUT TCP_TIMEWAIT_LEN 而阿里内核支持修改TIME_WAIT时间： net.ipv4.tcp_tw_timeout 然后找了两台机器做对比，用sysctl命令查看。杭州机房的机器： sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 3 上海机房的机器： $sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 60 原因很明显，上海机器的设置为60S。 ","date":"2014-07-01","objectID":"/time-wait/:0:0","tags":["linux","networking"],"title":"应用关闭后占用端口时间过长的问题","uri":"/time-wait/"},{"categories":["tech"],"content":"遇到性能问题怎么分析定位？这个问题太难回答了，各种底层环境、依赖系统、业务场景，怎么可能有统一的答案。于是产生了各种分析性能问题的“流派”。两个典型的 ANTI-METHODOLOGIES： blame-someone-else 使用此方法的人遵循下列步骤： 找到一个不是他负责的系统或环境 假定问题和这个组件有关 将问题转交个负责这个组件的团队 如果证明是错误的，重复步骤1 路灯法 没有系统的方法论，只是使用自己擅长的工具去观察，而不管问题到底出现在哪儿。就像丢了钥匙的人去路灯下寻找，仅仅是因为路灯下比较亮。这种行为被称为路灯效应。 相信很多同学已经脑补出上述的两个场景，他们的行为模式让人抓狂。于是有聪明人总结出了《The USE Method》。USE是Utilization，Saturation 和 Errors的缩写，简单说USE是一套分析系统性能问题的方法论，具体表现为一个checklist，分析过程就是对照checklist一项项检查，希望能快速定位瓶颈资源或错误。 初看这个方法感觉有点太简单了吧，这也能称为方法论？不过这确实体现出了老外的做事风格，任何事情都会去做定量分析，力求逻辑完整。而我们往往讳莫高深的一笑，只可意会不可言传。 简单介绍下USE，详细内容推荐看这篇《The USE Method》。USE的一句话总结： For every resource, check utilization, saturation, and errors. 术语解释 resource：CPU，内存，磁盘，网络等一切物理设备资源 utilization：资源利用率。例如CPU的资源利用率90% saturation：当资源繁忙时仍能接收新的任务，这些额外的任务一般都放入了等待队列。saturation就表现为队列的长度，例如CPU的平均运行队列为4（Linux上使用vmstat命令获得）。 errors：系统的错误报告数，例如TCP监听队列overflowed次数。 列出系统中的所有资源，然后逐项检查利用率、等待队列和错误数，就这么简单！下表是一个范例： resource type metric CPU utilization CPU utilization (either per-CPU or a system-wide average) CPU saturation run-queue length Memory capacity utilization available free memory (system-wide) Memory capacity saturation anonymous paging or thread swapping Network interface utilization RX/TX throughput / max bandwidth Storage Storage device I/O utilization device busy percent Storage device I/O saturation wait queue length Storage device I/O errors device errors (“soft”, “hard”, …) 对于资源测量数据的解读，作者给了一些建议，例如：资源利用率100%肯定表示该资源是系统瓶颈，70%以上的利用率就要引起足够的重视，一般IO设备利用率高于70%，响应时间将大幅上升。资源等待队列大于0意味着可能存在问题。资源的任何错误计数，都值得仔细调查，特别是当性能变差时，错误计数在上升。 要使用这个方法，你还需要一份完整的资源列表，一般的系统资源包括： CPUs: sockets, cores, hardware threads (virtual CPUs) Memory: capacity Network interfaces Storage devices: I/O, capacity Controllers: storage, network cards Interconnects: CPUs, memory, I/O 作者很厚道的按照每种操作系统给出了checklist，重点关注《USE Method: Linux Performance Checklist》，不仅列出了资源，而且告诉你如何进行测量。例如CPU运行队列的测量： system-wide: vmstat 1, “r” \u003e CPU count [2]; sar -q, “runq-sz” \u003e CPU count; dstat -p, “run” \u003e CPU count; per-process: /proc/PID/schedstat 2nd field (sched_info.run_delay); perf sched latency (shows “Average” and “Maximum” delay per-schedule); dynamic tracing, eg, SystemTap schedtimes.stp “queued(us)” 根据作者的实践经验，使用USE方法解决了80%的性能问题，只付出了5%的努力，当考虑了所有的资源，你不太可能忽视任何问题。简单有效！ ","date":"2014-06-07","objectID":"/use-method/:0:0","tags":["performance"],"title":"使用USE Method分析系统性能问题","uri":"/use-method/"},{"categories":["tech"],"content":"/proc是一个伪文件系统，可以像访问普通文件系统一样访问系统内部的数据结构，获取当前运行的进程、统计和硬件等各种信息。例如可以使用cat /proc/cpuinfo获取CPU信息。 /proc/sys/下的文件和子目录比较特别，它们对应的是系统内核参数，更改文件内容就意味着修改了相应的内核参数，可以简单的使用echo命令来完成修改： echo 1 \u003e /proc/sys/net/ipv4/tcp_syncookies 上面这个命令启用了TCP SYN Cookie保护。使用echo修改内核参数很方便，但是系统重启后这些修改都会消失，而且不方便配置参数的集中管理。/sbin/sysctl命令就是用来查看和修改内核参数的工具。sysctl -a会列出所有内核参数当前的配置信息，比遍历目录/proc/sys/方便多了。sysctl -w修改单个参数的配置，例如： sysctl -w net.ipv4.tcp_syncookies=1 和上面echo命令的效果一样。需要注意的是，要把目录分隔符斜杠/替换为点.，并省略proc.sys部分。 通过sysctl -w修改，还是没有解决重启后修改失效的问题。更常用的方式是，把需要修改的配置集中放在/etc/sysctl.conf文件中，使用sysctl -p重新加载配置使其生效。在系统启动阶段，init程序会运行/etc/rc.d/rc.sysinit脚本，其中包含了执行sysctl命令，并使用了/etc/sysctl.conf中的配置信息。因此放在/etc/sysctl.conf中的系统参数设置在重启后也同样生效，同时也便于集中管理修改过了哪些内核参数。 最后，哪里有比较完整的内核参数说明文档？我觉得kernel.org的文档比较全。例如我们常会遇到的网络内核参数，net.core 和 net.ipv4 。TCP相关的参数，也可以通过man文档了解。 ","date":"2014-05-30","objectID":"/linux-kernel-parameter/:0:0","tags":["linux"],"title":"Linux内核参数的配置方法","uri":"/linux-kernel-parameter/"},{"categories":["tech"],"content":"Git Feature Branch Workflow Zion项目我们采用Feature Branch Workflow，即每个特性在branch中开发，master始终保持稳定。特性开发完成，需提交pull request，接受其他成员的code review，同时可以在PR中围绕该特性进行讨论，PR记录了开发过程的细节。 由于是内部项目，我们没有使用fork机制，代码都维护在Github上的一个仓库：apusic/zion。在看具体的流程前，先有一个全局视图： ","date":"0001-01-01","objectID":"/github-workflow/:0:0","tags":["git"],"title":"Git Feature Branch Workflow","uri":"/github-workflow/"},{"categories":["tech"],"content":"基本工作流程 从远程clone respository git clone https://github.com/apusic/zion.git 创建特性分支 首先让本地的master处于最新状态： git fetch git checkout master git rebase origin/master 创建分支 git checkout -b myfeature 在分支上进行开发 git status # View the state of the repo git add # Stage a file git commit # Commit a file 进行一个功能特性开发时，可以多次提交到本地仓库Repository，不必每次commit都push到远程仓库Remote。 开发过程中，保持分支和最新代码同步 # While on your myfeature branch. git fetch git rebase origin/master 关于rebase的详细说明，请参考 The “Git Branching - Rebasing” chapter from the Pro Git book. 后面会单独介绍rebase冲突的处理。 将分支发布到中心仓库 所有改动都提交后，执行： git push -f origin myfeature 创建pull request 访问项目主页，点击Compare \u0026 pull request创建pull request code review \u0026 discussions 可以要求一个或两个项目成员进行review，也可以围绕该特性进行讨论。 Merge pull requests 根据项目的配置，pull requests在merge进master之前要满足一些条件，例如至少两个成员review，通过集成测试等。 所有的检查都通过后，这个pull request就可以merge了。详细的操作参见 Merging a pull request 这里有三个merge选型：Merge pull request，Squash and merge，Rebase and merge，关于它们的区别请参考GitHub的帮助。 一般建议选择Squash and merge。 至此，一个特性就开发完成了。 删除分支 已经完成merged的 pull requests 关联的分支可以在GitHub删除，详细的操作步骤参见GitHub文档 Deleting and restoring branches in a pull request ","date":"0001-01-01","objectID":"/github-workflow/:1:0","tags":["git"],"title":"Git Feature Branch Workflow","uri":"/github-workflow/"},{"categories":["tech"],"content":"冲突处理 上面的流程中，在保持分支和最新代码同步时，最有可能产生冲突。 rebase提示冲突，会列出冲突文件，执行下列步骤： 手工解决冲突 git add \u003csome-file\u003e 将发生冲突的文件放回index区 git rebase --continue 继续进行rebase 提示rebase成功 在Merge pull requests过程中也可能产生冲突，可以在GitHub的界面上解决冲突，详细的操作轻参考Addressing merge conflicts。 如果冲突较多，建议先在客户端执行rebase，按照上面的步骤解决完冲突，再进行Merge pull requests。 ","date":"0001-01-01","objectID":"/github-workflow/:2:0","tags":["git"],"title":"Git Feature Branch Workflow","uri":"/github-workflow/"}]