[{"categories":["tech"],"content":"DNS（Domain Name System）简单说就是一个名称到IP地址的映射，使用容易记住的域名代替IP地址。基本原理就不讲了，网上的文章很多。 了解了基本原理，你就可以使用dig(Domain Information Groper）命令进行探索。当对淘宝的几个域名进行dig时，你发现事情并不像想像的那么简单。 为了使大家的输出一致，我在dig命令中显示的指定了DNS服务器（@222.172.200.68 云南电信DNS）。 dig @222.172.200.68 login.alibaba-inc.com dig @222.172.200.68 guang.taobao.com dig @222.172.200.68 item.taobao.com 你会发现对于域名的查询，都不是直接返回IP地址这么简单，而是经过了神奇的CNAME。一般文档在介绍CNAME时只是说可以给一个域名指定别名（alias），其实这是DNS运维非常重要的手段，使得DNS配置具有一定的灵活性和可扩展性。结合上面三个域名的解析说一下。先给一张高大上的图，是按照我自己的理解画的，不一定完全正确:) 图上分了三层，最上层是常规的DNS解析过程，用户通过local DNS做递归查询，最终定位到taobao.com权威DNS服务器。 中间层可以称为GSLB（Global Server Load Balancing），作用是提供域名的智能解析，根据一定的策略返回结果。淘系目前有三套GSLB： F5 GTM：F5的硬件设备，基本已经被淘汰，全部替换为自研软件。GTM功能强大，但对用户而言是黑盒，性能一般价格昂贵。早期淘宝CDN智能调度就是基于F5 GTM做的。 ADNS：阿里自研权威DNS，替换GTM。ADNS很牛逼，可惜资料太少。 Pharos：阿里CDN的大脑，实现CDN流量精确，稳定，安全的调度。 taobao.com权威DNS服务器会根据不用的域名，CNAME到不同的GSLB做智能调度。CNAME的作用有点类似请求分发，taobao.com权威DNS服务器将域名解析请求转交给下一层域名服务器处理。 最下层是应用层，提供真正的服务。 现在再看看这三个域名的解析。 login.alibaba-inc.com 被转交给了GTM做智能解析，GTM通过返回不同机房的VIP做流量调度，用户的请求最终经过LVS到达我们的应用。 guang.taobao.com的解析过程和login.alibaba-inc.com类似，只不过智能调度换成了ADNS。 item.taobao.com有点小复杂。我们都知道CDN是做静态资源加速的，像这样的静态资源域名img04.taobaocdn.com会由Pharos解析调度，为用户返回就近的CDN节点。但什么时候动态内容也经过CDN代理了？这就是高大上的统一接入层。简单说下过程：item.taobao.com通过Pharos的智能调度，返回给用户就近的CDN节点。当用户的请求到达CDN节点时，这个节点会为动态内容的域名选择合适的后端服务，相当于每次都做回源处理。这个CDN节点可以理解为用户请求的代理。CDN在选择后端服务时，会执行单元化、小淘宝等逻辑，将请求发送到正确的机房。请求到达机房后，先进入统一接入层，注意这里的后端应用不需要申请VIP，IP地址列表保存在VIPServer中。统一接入层从VIPServer中拿到后端应用的IP地址列表，进行请求分发。VIPServer的作用类似HSF的ConfigServer，可以大大减少应用VIP的数量。以后做单元化部署的域名都会接入统一接入层，将单元化的逻辑上推到了CDN节点。 貌似是讲清楚了，不过这个过程已经做了很大的简化，因为我也仅仅是了解个大概。作为业务开发重点关注的是最下面的Java应用，转岗到技术保障后，才发现有机会可以从全局了解网站架构，接触到网络、DNS、CDN、LVS\u0026VIP等等基础设施。 ","date":"2014-09-10","objectID":"/dns/:0:0","tags":["linux","networking"],"title":"从开发角度看DNS","uri":"/dns/"},{"categories":["tech"],"content":"在Linux上做网络应用的性能优化时，一般都会对TCP相关的内核参数进行调节，特别是和缓冲、队列有关的参数。网上搜到的文章会告诉你需要修改哪些参数，但我们经常是知其然而不知其所以然，每次照抄过来后，可能很快就忘记或混淆了它们的含义。本文尝试总结TCP队列缓冲相关的内核参数，从协议栈的角度梳理它们，希望可以更容易的理解和记忆。注意，本文内容均来源于参考文档，没有去读相关的内核源码做验证，不能保证内容严谨正确。作为Java程序员没读过内核源码是硬伤。 下面我以server端为视角，从连接建立、数据包接收和数据包发送这3条路径对参数进行归类梳理。 ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:0","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"一、连接建立 简单看下连接的建立过程，客户端向server发送SYN包，server回复SYN＋ACK，同时将这个处于SYN_RECV状态的连接保存到半连接队列。客户端返回ACK包完成三次握手，server将ESTABLISHED状态的连接移入accept队列，等待应用调用accept()。 可以看到建立连接涉及两个队列： 半连接队列，保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置 accept队列，保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn, backlog)。其中backlog是我们创建ServerSocket(int port,int backlog)时指定的参数，最终会传递给listen方法： #include \u003csys/socket.h\u003e int listen(int sockfd, int backlog); 如果我们设置的backlog大于net.core.somaxconn，accept队列的长度将被设置为net.core.somaxconn 另外，为了应对SYN flooding（即客户端只发送SYN包发起握手而不回应ACK完成连接建立，填满server端的半连接队列，让它无法处理正常的握手请求），Linux实现了一种称为SYN cookie的机制，通过net.ipv4.tcp_syncookies控制，设置为1表示开启。简单说SYN cookie就是将连接信息编码在ISN(initial sequence number)中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。对于一去不复返的客户端握手，不理它就是了。 ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:1","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"二、数据包的接收 先看看接收数据包经过的路径： 数据包的接收，从下往上经过了三层：网卡驱动、系统内核空间，最后到用户态空间的应用。Linux内核使用sk_buff(socket kernel buffers)数据结构描述一个数据包。当一个新的数据包到达，NIC（network interface controller）调用DMA engine，通过Ring Buffer将数据包放置到内核内存区。Ring Buffer的大小固定，它不包含实际的数据包，而是包含了指向sk_buff的描述符。当Ring Buffer满的时候，新来的数据包将给丢弃。一旦数据包被成功接收，NIC发起中断，由内核的中断处理程序将数据包传递给IP层。经过IP层的处理，数据包被放入队列等待TCP层处理。每个数据包经过TCP层一系列复杂的步骤，更新TCP状态机，最终到达recv Buffer，等待被应用接收处理。有一点需要注意，数据包到达recv Buffer，TCP就会回ACK确认，既TCP的ACK表示数据包已经被操作系统内核收到，但并不确保应用层一定收到数据（例如这个时候系统crash），因此一般建议应用协议层也要设计自己的ACK确认机制。 上面就是一个相当简化的数据包接收流程，让我们逐层看看队列缓冲有关的参数。 网卡Bonding模式 当主机有1个以上的网卡时，Linux会将多个网卡绑定为一个虚拟的bonded网络接口，对TCP/IP而言只存在一个bonded网卡。多网卡绑定一方面能够提高网络吞吐量，另一方面也可以增强网络高可用。Linux支持7种Bonding模式： - `Mode 0 (balance-rr)` Round-robin策略，这个模式具备负载均衡和容错能力 - `Mode 1 (active-backup)` 主备策略，在绑定中只有一个网卡被激活，其他处于备份状态 - `Mode 2 (balance-xor)` XOR策略，通过源MAC地址与目的MAC地址做异或操作选择slave网卡 - `Mode 3 (broadcast)` 广播，在所有的网卡上传送所有的报文 - `Mode 4 (802.3ad)` IEEE 802.3ad 动态链路聚合。创建共享相同的速率和双工模式的聚合组 - `Mode 5 (balance-tlb)` Adaptive transmit load balancing - `Mode 6 (balance-alb)` Adaptive load balancing 详细的说明参考内核文档Linux Ethernet Bonding Driver HOWTO。我们可以通过cat /proc/net/bonding/bond0查看本机的Bonding模式： 一般很少需要开发去设置网卡Bonding模式，自己实验的话可以参考这篇文档 网卡多队列及中断绑定 随着网络的带宽的不断提升，单核CPU已经不能满足网卡的需求，这时通过多队列网卡驱动的支持，可以将每个队列通过中断绑定到不同的CPU核上，充分利用多核提升数据包的处理能力。 首先查看网卡是否支持多队列，使用lspci -vvv命令，找到Ethernet controller项： 如果有MSI-X， Enable+ 并且Count \u003e 1，则该网卡是多队列网卡。 然后查看是否打开了网卡多队列。使用命令cat /proc/interrupts，如果看到eth0-TxRx-0表明多队列支持已经打开： 最后确认每个队列是否绑定到不同的CPU。cat /proc/interrupts查询到每个队列的中断号，对应的文件/proc/irq/${IRQ_NUM}/smp_affinity为中断号IRQ_NUM绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核： ``` （00000001）代表CPU0 （00000010）代表CPU1 （00000011）代表CPU0和CPU1 ``` 如果绑定的不均衡，可以手工设置，例如： ``` echo \"1\" \u003e /proc/irq/99/smp_affinity echo \"2\" \u003e /proc/irq/100/smp_affinity echo \"4\" \u003e /proc/irq/101/smp_affinity echo \"8\" \u003e /proc/irq/102/smp_affinity echo \"10\" \u003e /proc/irq/103/smp_affinity echo \"20\" \u003e /proc/irq/104/smp_affinity echo \"40\" \u003e /proc/irq/105/smp_affinity echo \"80\" \u003e /proc/irq/106/smp_affinity ``` Ring Buffer Ring Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。 可以使用ethtool -g eth0查看当前Ring Buffer的设置： 上面的例子接收队列为4096，传输队列为256。可以通过ifconfig观察接收和传输队列的运行状况： RX errors：收包总的错误数 RX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。 RX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。 当dropped数量持续增加，建议增大Ring Buffer，使用ethtool -G进行设置。 Input Packet Queue(数据包接收队列) 当接收数据包的速率大于内核TCP处理包的速率，数据包将会缓冲在TCP层之前的队列中。接收队列的长度由参数net.core.netdev_max_backlog设置。 recv Buffer recv buffer是调节TCP性能的关键参数。BDP(Bandwidth-delay product，带宽延迟积) 是网络的带宽和与RTT(round trip time)的乘积，BDP的含义是任意时刻处于在途未确认的最大数据量。RTT使用ping命令可以很容易的得到。为了达到最大的吞吐量，recv Buffer的设置应该大于BDP，即recv Buffer \u003e= bandwidth * RTT。假设带宽是100Mbps，RTT是100ms，那么BDP的计算如下： BDP = 100Mbps * 100ms = (100 / 8) * (100 / 1000) = 1.25MB Linux在2.6.17以后增加了recv Buffer自动调节机制，recv buffer的实际大小会自动在最小值和最大值之间浮动，以期找到性能和资源的平衡点，因此大多数情况下不建议将recv buffer手工设置成固定值。 当net.ipv4.tcp_moderate_rcvbuf设置为1时，自动调节机制生效，每个TCP连接的recv Buffer由下面的3元数组指定： net.ipv4.tcp_rmem = \u003cMIN\u003e \u003cDEFAULT\u003e \u003cMAX\u003e 最初recv buffer被设置为，同时这个缺省值会覆盖net.core.rmem_default的设置。随后recv buffer根据实际情况在最大值和最小值之间动态调节。在缓冲的动态调优机制开启的情况下，我们将net.ipv4.tcp_rmem的最大值设置为BDP。 当net.ipv4.tcp_moderate_rcvbuf被设置为0，或者设置了socket选项SO_RCVBUF，缓冲的动态调节机制被关闭。recv buffer的缺省值由net.core.rmem_default设置，但如果设置了net.ipv4.tcp_rmem，缺省值则被\u003cDEFAULT\u003e覆盖。可以通过系统调用setsockopt()设置recv buffer的最大值为net.core.rmem_max。在缓冲动态调节机制关闭的情况下，建议把缓冲的缺省值设置为BDP。 注意这里还有一个细节，缓冲除了保存接收的数据本身，还需要一部分空间保存socket数据结构等额外信息。因此上面讨论的recv buffer最佳值仅仅等于BDP是不够的，还需要考虑保存socket等额外信息的开销。Linux根据参数net.ipv4.tcp_adv_win_scale计算额外开销的大小： Buffer / 2tcp_adv_win_scale 如果net.ipv4.tcp_adv_win_scale的值为1，则二分之一的缓冲空间用来做额外开销，如果为2的话，则四分之一缓冲空间用来做额外开销。因此recv buffer的最佳值应该设置为： BDP / (1 – 1 / 2tcp_adv_win_scale) ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:2","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"三、数据包的发送 发送数据包经过的路径： 和接收数据的路径相反，数据包的发送从上往下也经过了三层：用户态空间的应用、系统内核空间、最后到网卡驱动。应用先将数据写入TCP send buffer，TCP层将send buffer中的数据构建成数据包转交给IP层。IP层会将待发送的数据包放入队列QDisc(queueing discipline)。数据包成功放入QDisc后，指向数据包的描述符sk_buff被放入Ring Buffer输出队列，随后网卡驱动调用DMA engine将数据发送到网络链路上。 同样我们逐层来梳理队列缓冲有关的参数。 send Buffer 同recv Buffer类似，和send Buffer有关的参数如下： net.ipv4.tcp_wmem = \u003cMIN\u003e \u003cDEFAULT\u003e \u003cMAX\u003e net.core.wmem_default net.core.wmem_max 发送端缓冲的自动调节机制很早就已经实现，并且是无条件开启，没有参数去设置。如果指定了tcp_wmem，则net.core.wmem_default被tcp_wmem的覆盖。send Buffer在tcp_wmem的最小值和最大值之间自动调节。如果调用setsockopt()设置了socket选项SO_SNDBUF，将关闭发送端缓冲的自动调节机制，tcp_wmem将被忽略，SO_SNDBUF的最大值由net.core.wmem_max限制。 QDisc QDisc（queueing discipline ）位于IP层和网卡的ring buffer之间。我们已经知道，ring buffer是一个简单的FIFO队列，这种设计使网卡的驱动层保持简单和快速。而QDisc实现了流量管理的高级功能，包括流量分类，优先级和流量整形（rate-shaping）。可以使用tc命令配置QDisc。 QDisc的队列长度由txqueuelen设置，和接收数据包的队列长度由内核参数net.core.netdev_max_backlog控制所不同，txqueuelen是和网卡关联，可以用ifconfig命令查看当前的大小： 使用ifconfig调整txqueuelen的大小： ifconfig eth0 txqueuelen 2000 Ring Buffer 和数据包的接收一样，发送数据包也要经过Ring Buffer，使用ethtool -g eth0查看： 其中TX项是Ring Buffer的传输队列，也就是发送队列的长度。设置也是使用命令ethtool -G。 TCP Segmentation和Checksum Offloading 操作系统可以把一些TCP/IP的功能转交给网卡去完成，特别是Segmentation(分片)和checksum的计算，这样可以节省CPU资源，并且由硬件代替OS执行这些操作会带来性能的提升。 一般以太网的MTU（Maximum Transmission Unit）为1500 bytes，假设应用要发送数据包的大小为7300bytes，MTU1500字节 － IP头部20字节 － TCP头部20字节＝有效负载为1460字节，因此7300字节需要拆分成5个segment： Segmentation(分片)操作可以由操作系统移交给网卡完成，虽然最终线路上仍然是传输5个包，但这样节省了CPU资源并带来性能的提升： 可以使用ethtool -k eth0查看网卡当前的offloading情况： 上面这个例子checksum和tcp segmentation的offloading都是打开的。如果想设置网卡的offloading开关，可以使用ethtool -K(注意K是大写)命令，例如下面的命令关闭了tcp segmentation offload： sudo ethtool -K eth0 tso off 网卡多队列和网卡Bonding模式 在数据包的接收过程中已经介绍过了。 至此，终于梳理完毕。整理TCP队列相关参数的起因是最近在排查一个网络超时问题，原因还没有找到，产生的“副作用”就是这篇文档。再想深入解决这个问题可能需要做TCP协议代码的profile，需要继续学习，希望不久的将来就可以再写文档和大家分享了。 参考文档 Queueing in the Linux Network Stack TCP Implementation in Linux: A Brief Tutorial Impact of Bandwidth Delay Product on TCP Throughput Java程序员也应该知道的系统知识系列之网卡 ","date":"2014-08-16","objectID":"/linux-tcp-queue/:0:3","tags":["linux","networking","performance"],"title":"Linux TCP队列相关参数的总结","uri":"/linux-tcp-queue/"},{"categories":["tech"],"content":"Java中通过Socket.setSoLinger设置SO_LINGER选项，有三种组合形式： Socket.setSoLinger(false, linger) 设置为false，这时linger值被忽略。摘自unix network programming： The default action of close with a TCP socket is to mark the socket as closed and return to the process immediately. The socket descriptor is on longer usable by the process: it can’t be used as an argument to read or write. TCP will try to send any data that is already queued to be sent to the other end, and after this occurs, the normal TCP connection termination sequence takes place. 如果设置为false，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入TIME_WAIT状态。 我们可以写个演示程序，客户端发送较大的数据包后，立刻调用close，而server端将Receive Buffer设置的很小。close会立即返回，客户端的Java进程结束，但是当我们用tcpdump/Wireshark抓包会发现，操作系统正在帮你发送数据，内核缓冲区中的数据发送完毕后，发送FIN包关闭连接。 Socket.setSoLinger(true, 0) TCP discards any data still remaining in the socket send buffer and sends an RST to the peer, not the normal four-packet connection termination sequence. 主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送RST，对端会收到java.net.SocketException: Connection reset异常。同样使用tcpdump抓包可以很容易观察到。 另外有些人会用这种方式解决主动关闭放方有大量TIME_WAIT状态连接的问题，因为发送完RST后，连接立即销毁，不会停留在TIME_WAIT状态。一般不建议这么做，除非你有合适的理由： If the a client of your server application misbehaves (times out, returns invalid data, etc.) an abortive close makes sense to avoid being stuck in CLOSE_WAIT or ending up in the TIME_WAIT state. If you must restart your server application which currently has thousands of client connections you might consider setting this socket option to avoid thousands of server sockets in TIME_WAIT (when calling close() from the server end) as this might prevent the server from getting available ports for new client connections after being restarted. On page 202 in the aforementioned book it specifically says: “There are certain circumstances which warrant using this feature to send an abortive close. One example is an RS-232 terminal server, which might hang forever in CLOSE_WAIT trying to deliver data to a stuck terminal port, but would properly reset the stuck port if it got an RST to discard the pending data.” Socket.setSoLinger(true, linger \u003e 0) if there is any data still remaining in the socket send buffer, the process will sleep when calling close() until either all the data is sent and acknowledged by the peer or the configured linger timer expires. if the linger time expires before the remaining data is sent and acknowledged, close returns EWOULDBLOCK and any remaining data in the send buffer is discarded. 如果SO_LINGER选项生效，并且超时设置大于零，调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况： 数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK） 超时，未发送完成的数据被丢弃，连接发送RST进行非正常关闭 类似的我们也可以构造demo观察这种场景。客户端发送较大的数据包，server端将Receive Buffer设置的很小。设置linger为1，调用close时等待1秒。注意SO_LINGER的单位为秒，好多人被坑过。假设close后1秒内缓冲区中的数据发送不完，使用tcpdump/Wireshark可以观察到客户端发送RST包，服务端收到java.net.SocketException: Connection reset异常。 最后，在使用NIO时，最好不设置SO_LINGER，以后会再写一篇文章分析。 ","date":"2014-08-10","objectID":"/so-linger/:0:0","tags":["java","networking"],"title":"TCP `SO_LINGER` 选项对Socket.close的影响","uri":"/so-linger/"},{"categories":["tech"],"content":"早上毕玄转给我一个问题，vsearch在上海机房部署的应用，在应用关闭后，端口释放的时间要比杭州机房的时间长。 TCP的基本知识，主动关闭连接的一方会处于TIME_WAIT状态，并停留两倍的MSL（Maximum segment lifetime）时长。 那就检查一下MSL的设置。网上有很多文章说，可以通过设置net.ipv4.tcp_fin_timeout来控制MSL。其实这有点误导人。查看Linux kernel的文档 ，发现tcp_fin_timeout是指停留在FIN_WAIT_2状态的时间： tcp_fin_timeout - INTEGER The length of time an orphaned (no longer referenced by any application) connection will remain in the FIN_WAIT_2 state before it is aborted at the local end. While a perfectly valid “receive only” state for an un-orphaned connection, an orphaned connection in FIN_WAIT_2 state could otherwise wait forever for the remote to close its end of the connection. Default: 60 seconds 幸好这个问题原先在内部请教过： sysctl调节不了，只能调节复用和回收。 以前改小是改下面文件，重新编译内核的。 grep -i timewait_len /usr/src/kernels/2.6.32-220.el6.x86_64/include/net/tcp.h define TCP_TIMEWAIT_LEN (60HZ) / how long to wait to destroy TIME-WAIT define TCP_FIN_TIMEOUT TCP_TIMEWAIT_LEN 而阿里内核支持修改TIME_WAIT时间： net.ipv4.tcp_tw_timeout 然后找了两台机器做对比，用sysctl命令查看。杭州机房的机器： sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 3 上海机房的机器： $sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 60 原因很明显，上海机器的设置为60S。 ","date":"2014-07-01","objectID":"/time-wait/:0:0","tags":["linux","networking"],"title":"应用关闭后占用端口时间过长的问题","uri":"/time-wait/"},{"categories":["tech"],"content":"遇到性能问题怎么分析定位？这个问题太难回答了，各种底层环境、依赖系统、业务场景，怎么可能有统一的答案。于是产生了各种分析性能问题的“流派”。两个典型的 ANTI-METHODOLOGIES： blame-someone-else 使用此方法的人遵循下列步骤： 找到一个不是他负责的系统或环境 假定问题和这个组件有关 将问题转交个负责这个组件的团队 如果证明是错误的，重复步骤1 路灯法 没有系统的方法论，只是使用自己擅长的工具去观察，而不管问题到底出现在哪儿。就像丢了钥匙的人去路灯下寻找，仅仅是因为路灯下比较亮。这种行为被称为路灯效应。 相信很多同学已经脑补出上述的两个场景，他们的行为模式让人抓狂。于是有聪明人总结出了《The USE Method》。USE是Utilization，Saturation 和 Errors的缩写，简单说USE是一套分析系统性能问题的方法论，具体表现为一个checklist，分析过程就是对照checklist一项项检查，希望能快速定位瓶颈资源或错误。 初看这个方法感觉有点太简单了吧，这也能称为方法论？不过这确实体现出了老外的做事风格，任何事情都会去做定量分析，力求逻辑完整。而我们往往讳莫高深的一笑，只可意会不可言传。 简单介绍下USE，详细内容推荐看这篇《The USE Method》。USE的一句话总结： For every resource, check utilization, saturation, and errors. 术语解释 resource：CPU，内存，磁盘，网络等一切物理设备资源 utilization：资源利用率。例如CPU的资源利用率90% saturation：当资源繁忙时仍能接收新的任务，这些额外的任务一般都放入了等待队列。saturation就表现为队列的长度，例如CPU的平均运行队列为4（Linux上使用vmstat命令获得）。 errors：系统的错误报告数，例如TCP监听队列overflowed次数。 列出系统中的所有资源，然后逐项检查利用率、等待队列和错误数，就这么简单！下表是一个范例： resource type metric CPU utilization CPU utilization (either per-CPU or a system-wide average) CPU saturation run-queue length Memory capacity utilization available free memory (system-wide) Memory capacity saturation anonymous paging or thread swapping Network interface utilization RX/TX throughput / max bandwidth Storage Storage device I/O utilization device busy percent Storage device I/O saturation wait queue length Storage device I/O errors device errors (“soft”, “hard”, …) 对于资源测量数据的解读，作者给了一些建议，例如：资源利用率100%肯定表示该资源是系统瓶颈，70%以上的利用率就要引起足够的重视，一般IO设备利用率高于70%，响应时间将大幅上升。资源等待队列大于0意味着可能存在问题。资源的任何错误计数，都值得仔细调查，特别是当性能变差时，错误计数在上升。 要使用这个方法，你还需要一份完整的资源列表，一般的系统资源包括： CPUs: sockets, cores, hardware threads (virtual CPUs) Memory: capacity Network interfaces Storage devices: I/O, capacity Controllers: storage, network cards Interconnects: CPUs, memory, I/O 作者很厚道的按照每种操作系统给出了checklist，重点关注《USE Method: Linux Performance Checklist》，不仅列出了资源，而且告诉你如何进行测量。例如CPU运行队列的测量： system-wide: vmstat 1, “r” \u003e CPU count [2]; sar -q, “runq-sz” \u003e CPU count; dstat -p, “run” \u003e CPU count; per-process: /proc/PID/schedstat 2nd field (sched_info.run_delay); perf sched latency (shows “Average” and “Maximum” delay per-schedule); dynamic tracing, eg, SystemTap schedtimes.stp “queued(us)” 根据作者的实践经验，使用USE方法解决了80%的性能问题，只付出了5%的努力，当考虑了所有的资源，你不太可能忽视任何问题。简单有效！ ","date":"2014-06-07","objectID":"/use-method/:0:0","tags":["performance"],"title":"使用USE Method分析系统性能问题","uri":"/use-method/"},{"categories":["tech"],"content":"/proc是一个伪文件系统，可以像访问普通文件系统一样访问系统内部的数据结构，获取当前运行的进程、统计和硬件等各种信息。例如可以使用cat /proc/cpuinfo获取CPU信息。 /proc/sys/下的文件和子目录比较特别，它们对应的是系统内核参数，更改文件内容就意味着修改了相应的内核参数，可以简单的使用echo命令来完成修改： echo 1 \u003e /proc/sys/net/ipv4/tcp_syncookies 上面这个命令启用了TCP SYN Cookie保护。使用echo修改内核参数很方便，但是系统重启后这些修改都会消失，而且不方便配置参数的集中管理。/sbin/sysctl命令就是用来查看和修改内核参数的工具。sysctl -a会列出所有内核参数当前的配置信息，比遍历目录/proc/sys/方便多了。sysctl -w修改单个参数的配置，例如： sysctl -w net.ipv4.tcp_syncookies=1 和上面echo命令的效果一样。需要注意的是，要把目录分隔符斜杠/替换为点.，并省略proc.sys部分。 通过sysctl -w修改，还是没有解决重启后修改失效的问题。更常用的方式是，把需要修改的配置集中放在/etc/sysctl.conf文件中，使用sysctl -p重新加载配置使其生效。在系统启动阶段，init程序会运行/etc/rc.d/rc.sysinit脚本，其中包含了执行sysctl命令，并使用了/etc/sysctl.conf中的配置信息。因此放在/etc/sysctl.conf中的系统参数设置在重启后也同样生效，同时也便于集中管理修改过了哪些内核参数。 最后，哪里有比较完整的内核参数说明文档？我觉得kernel.org的文档比较全。例如我们常会遇到的网络内核参数，net.core 和 net.ipv4 。TCP相关的参数，也可以通过man文档了解。 ","date":"2014-05-30","objectID":"/linux-kernel-parameter/:0:0","tags":["linux"],"title":"Linux内核参数的配置方法","uri":"/linux-kernel-parameter/"}]